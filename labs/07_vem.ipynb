{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S5M08pFAJxy"
      },
      "source": [
        "# Variational EM\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/09c7213d-d3ef-4c84-8321-07ab84a365af/fx1.jpg\" />\n",
        "</p>\n",
        "\n",
        "<!-- ![](https://els-jbs-prod-cdn.jbs.elsevierhealth.com/cms/attachment/09c7213d-d3ef-4c84-8321-07ab84a365af/fx1.jpg) -->\n",
        "\n",
        "In this lab, we will implement a variational expectation-maximization (EM) algorithm to fit a latent variable model with both discrete and continuous states. We'll use a mean field approximation, which we'll fit using coordinate ascent variational inference (CAVI). Then we'll test it out on neural activity traces extracted from calcium imaging of the worm _C. elegans_ by Kato et al (2015), in their paper on low dimensional dynamics of whole brain activity.\n",
        "\n",
        "We won't implement variational EM for full-blown switching linear dynamical systems, as described in clas. Instead, we'll work on a simpler model without time dependencies, which reduces to a mixture of factor analysis models. Once we've done so, you'll understand how the main fitting algorithms for SLDS work under the hood!\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "Kato, Saul, Harris S. Kaplan, Tina Schrödel, Susanne Skora, Theodore H. Lindsay, Eviatar Yemini, Shawn Lockery, and Manuel Zimmer. 2015. “Global Brain Dynamics Embed the Motor Command Sequence of Caenorhabditis Elegans.” Cell 163 (3): 656–69.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBtV5wMQDDfP"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5lkccXmMxZJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.io import loadmat\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm.auto import trange\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.distributions import Categorical, MultivariateNormal, Normal, \\\n",
        "    LowRankMultivariateNormal, kl_divergence\n",
        "\n",
        "device = torch.device('cpu')\n",
        "dtype = torch.float32\n",
        "\n",
        "# Helper function to convert between numpy arrays and tensors\n",
        "to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n",
        "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fDxAL0XkRVGR",
        "tags": [
          "hide-cell"
        ]
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions (run this cell!)\n",
        "\n",
        "import numpy as onp\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "kato_files = [\"TS20140715e_lite-1_punc-31_NLS3_2eggs_56um_1mMTet_basal_1080s.mat\",\n",
        "              \"TS20140715f_lite-1_punc-31_NLS3_3eggs_56um_1mMTet_basal_1080s.mat\",\n",
        "              \"TS20140905c_lite-1_punc-31_NLS3_AVHJ_0eggs_1mMTet_basal_1080s.mat\",\n",
        "              \"TS20140926d_lite-1_punc-31_NLS3_RIV_2eggs_1mMTet_basal_1080s.mat\",\n",
        "              \"TS20141221b_THK178_lite-1_punc-31_NLS3_6eggs_1mMTet_basal_1080s.mat\"]\n",
        "\n",
        "kato_dir = '.'\n",
        "\n",
        "# Set notebook plotting defaults\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "# initialize a color palette for plotting\n",
        "palette = sns.xkcd_palette([\"light blue\",   # forward\n",
        "                            \"navy\",         # slow\n",
        "                            \"orange\",       # dorsal turn\n",
        "                            \"yellow\",       # ventral turn\n",
        "                            \"red\",          # reversal 1\n",
        "                            \"pink\",         # reversal 2\n",
        "                            \"green\",        # sustained reversal\n",
        "                            \"greyish\"])     # no state\n",
        "\n",
        "def load_kato_labels():\n",
        "    zimmer_state_labels = \\\n",
        "        loadmat(os.path.join(\n",
        "            kato_dir,\n",
        "            \"sevenStateColoring.mat\"))\n",
        "    return zimmer_state_labels\n",
        "\n",
        "def load_kato_key():\n",
        "    data = load_kato_labels()\n",
        "    key = data[\"sevenStateColoring\"][\"key\"][0,0][0]\n",
        "    key = [str(k)[2:-2] for k in key]\n",
        "    return key\n",
        "\n",
        "\n",
        "def _get_neuron_names(neuron_ids_1, neuron_ids_2, worm_name):\n",
        "    # Remove the neurons that are not uniquely identified\n",
        "    def check_label(neuron_name):\n",
        "        if neuron_name is None:\n",
        "            return False\n",
        "        if neuron_name == \"---\":\n",
        "            return False\n",
        "\n",
        "        neuron_index = onp.where(neuron_ids_1 == neuron_name)[0]\n",
        "        if len(neuron_index) != 1:\n",
        "            return False\n",
        "\n",
        "        if neuron_ids_2[neuron_index[0]] is not None:\n",
        "            return False\n",
        "\n",
        "        # Make sure it doesn't show up in the second neuron list\n",
        "        if len(onp.where(neuron_ids_2 == neuron_name)[0]) > 0:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    final_neuron_names = []\n",
        "    for i, neuron_name in enumerate(neuron_ids_1):\n",
        "        if check_label(neuron_name):\n",
        "            final_neuron_names.append(neuron_name)\n",
        "        else:\n",
        "            final_neuron_names.append(\"{}_neuron{}\".format(worm_name, i))\n",
        "\n",
        "    return final_neuron_names\n",
        "\n",
        "\n",
        "def load_kato(index, sample_rate=3, name=\"unnamed\"):\n",
        "    filename = os.path.join(kato_dir, kato_files[index])\n",
        "    zimmer_data = loadmat(filename)\n",
        "\n",
        "    # Get the neuron names\n",
        "    neuron_ids = zimmer_data[\"wbData\"]['NeuronIds'][0, 0][0]\n",
        "    neuron_ids_1 = onp.array(\n",
        "        list(map(lambda x: None if len(x[0]) == 0\n",
        "                            else str(x[0][0][0]),\n",
        "            neuron_ids)))\n",
        "\n",
        "    neuron_ids_2 = onp.array(\n",
        "        list(map(lambda x: None if x.size < 2 or x[0, 1].size == 0\n",
        "                            else str(x[0, 1][0]),\n",
        "            neuron_ids)))\n",
        "\n",
        "    all_neuron_names = _get_neuron_names(neuron_ids_1, neuron_ids_2, name)\n",
        "\n",
        "    # Get the calcium trace (corrected for bleaching)\n",
        "    t_smpl = onp.ravel(zimmer_data[\"wbData\"]['tv'][0, 0])\n",
        "    t_start = t_smpl[0]\n",
        "    t_stop = t_smpl[-1]\n",
        "    tt = onp.arange(t_start, t_stop, step=1./sample_rate)\n",
        "    def interp_data(xx, kind=\"linear\"):\n",
        "        f = interp1d(t_smpl, xx, axis=0, kind=kind)\n",
        "        return f(tt)\n",
        "        # return np.interp(tt, t_smpl, xx, axis=0)\n",
        "\n",
        "    dff = interp_data(zimmer_data[\"wbData\"]['deltaFOverF'][0, 0])\n",
        "    dff_bc = interp_data(zimmer_data[\"wbData\"]['deltaFOverF_bc'][0, 0])\n",
        "    dff_deriv = interp_data(zimmer_data[\"wbData\"]['deltaFOverF_deriv'][0, 0])\n",
        "\n",
        "    # Kato et al smoothed the derivative.  Let's just work with the first differences\n",
        "    # of the bleaching corrected and normalized dF/F\n",
        "    dff_bc_zscored = (dff_bc - dff_bc.mean(0)) / dff_bc.std(0)\n",
        "    dff_diff = onp.vstack((onp.zeros((1, dff_bc_zscored.shape[1])),\n",
        "                                onp.diff(dff_bc_zscored, axis=0)))\n",
        "\n",
        "    # Get the state sequence as labeled in Kato et al\n",
        "    # Interpolate to get at new time points\n",
        "    labels = load_kato_labels()\n",
        "    labels = labels[\"sevenStateColoring\"][\"dataset\"][0, 0]['stateTimeSeries']\n",
        "    states = interp_data(labels[0, index].ravel() - 1, kind=\"nearest\").astype(int)\n",
        "\n",
        "    # Only keep the neurons with names\n",
        "    has_name = onp.array([not name.startswith(\"unnamed\") for name in all_neuron_names])\n",
        "    y = dff_bc[:, has_name]\n",
        "    neuron_names = [name for name, valid in zip(all_neuron_names, has_name) if valid]\n",
        "\n",
        "    # Load the state names from Kato et al\n",
        "    state_names=load_kato_key()\n",
        "    return dict(neuron_names=neuron_names,\n",
        "                y=torch.tensor(y, dtype=torch.float32),\n",
        "                z_kato=torch.tensor(states),\n",
        "                state_names=state_names,\n",
        "                fps=3)\n",
        "\n",
        "\n",
        "def gradient_cmap(colors, nsteps=256, bounds=None):\n",
        "    # Make a colormap that interpolates between a set of colors\n",
        "    ncolors = len(colors)\n",
        "    if bounds is None:\n",
        "        bounds = onp.linspace(0,1,ncolors)\n",
        "\n",
        "    reds = []\n",
        "    greens = []\n",
        "    blues = []\n",
        "    alphas = []\n",
        "    for b,c in zip(bounds, colors):\n",
        "        reds.append((b, c[0], c[0]))\n",
        "        greens.append((b, c[1], c[1]))\n",
        "        blues.append((b, c[2], c[2]))\n",
        "        alphas.append((b, c[3], c[3]) if len(c) == 4 else (b, 1., 1.))\n",
        "\n",
        "    cdict = {'red': tuple(reds),\n",
        "             'green': tuple(greens),\n",
        "             'blue': tuple(blues),\n",
        "             'alpha': tuple(alphas)}\n",
        "\n",
        "    cmap = LinearSegmentedColormap('grad_colormap', cdict, nsteps)\n",
        "    return cmap\n",
        "\n",
        "\n",
        "def states_to_changepoints(z):\n",
        "    assert z.ndim == 1\n",
        "    z = onp.array(z)\n",
        "    return onp.concatenate(([0], 1 + onp.where(onp.diff(z))[0], [z.size - 1]))\n",
        "\n",
        "\n",
        "def plot_2d_continuous_states(x, z,\n",
        "                              colors=palette,\n",
        "                              ax=None,\n",
        "                              inds=(0,1),\n",
        "                              figsize=(2.5, 2.5),\n",
        "                              **kwargs):\n",
        "\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        ax = fig.add_subplot(111)\n",
        "\n",
        "    cps = states_to_changepoints(z)\n",
        "\n",
        "    # Color denotes our inferred latent discrete state\n",
        "    for cp_start, cp_stop in zip(cps[:-1], cps[1:]):\n",
        "        ax.plot(x[cp_start:cp_stop + 1, inds[0]],\n",
        "                x[cp_start:cp_stop + 1, inds[1]],\n",
        "                 '-', color=colors[z[cp_start]],\n",
        "                **kwargs)\n",
        "\n",
        "cmap = gradient_cmap(palette)\n",
        "\n",
        "\n",
        "def plot_elbos(avg_elbos, marginal_ll=None):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    if marginal_ll is not None:\n",
        "        axs[0].hlines(marginal_ll, 0, len(avg_elbos),\n",
        "                colors='k', linestyles=':', label=\"$\\log p(y \\mid \\Theta)$\")\n",
        "    axs[0].plot(avg_elbos, label=\"$\\mathcal{L}[q, \\Theta]$\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_xlabel(\"Iteration\")\n",
        "    axs[0].set_ylabel(\"ELBO\")\n",
        "\n",
        "    if marginal_ll is not None:\n",
        "        axs[1].hlines(marginal_ll, 1, len(avg_elbos),\n",
        "                colors='k', linestyles=':', label=\"$\\log p(y \\mid \\Theta)$\")\n",
        "    axs[1].plot(torch.arange(1, len(avg_elbos)), avg_elbos[1:],\n",
        "                label=\"$\\mathcal{L}[q, \\Theta]$\")\n",
        "    axs[1].set_xlabel(\"Iteration\")\n",
        "    axs[1].set_ylabel(\"ELBO\")\n",
        "\n",
        "    axs[2].plot(avg_elbos[1:] - avg_elbos[:-1])\n",
        "    axs[2].set_xlabel(\"Iteration\")\n",
        "    axs[2].set_ylabel(\"Change in ELBO\")\n",
        "    axs[2].hlines(0, 0, len(avg_elbos) - 1,\n",
        "                colors='k', linestyles=':')\n",
        "\n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIBIRr2QFzN3"
      },
      "source": [
        "## Part 0: Build the generative model\n",
        "\n",
        "To start, we'll consider a model that has both discrete and continuous latent variables, just like a switching linear dynamical system, but we'll get rid of the time dependencies. Let $z_t \\in \\{1,\\ldots, K\\}$ denote a discrete latent state, $x_t \\in \\mathbb{R}^D$ denote a continuous latent state, and $y_t \\in \\mathbb{R}^N$ denote an observed data point. The model is,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(z, x, y \\mid \\Theta) &= \\prod_{t=1}^T p(z_t \\mid \\Theta) \\, p(x_t \\mid z_t, \\Theta) \\, p(y_t \\mid x_t, \\Theta) \\\\\n",
        "&= \\prod_{t=1}^T \\mathrm{Cat}(z_t \\mid \\pi) \\, \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t}) \\, \\mathcal{N}(y_t \\mid C x_t + d, R)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where the parameters $\\Theta$ consist of,\n",
        "- $\\pi \\in \\Delta_K$, a distribution on discrete states\n",
        "- $b_k \\in \\mathbb{R}^D$, a mean for each discrete state\n",
        "- $Q_k \\in \\mathbb{R}^{D \\times D}$, a covariance for each discrete state\n",
        "- $C \\in \\mathbb{R}^{N \\times D}$, an _observation matrix_\n",
        "- $d \\in \\mathbb{R}^{N}$, an _observation bias_\n",
        "- $R = \\mathrm{diag}([r_1^2, \\ldots, r_N^2])$, a diagonal observation coariance matrix.\n",
        "\n",
        "This is called a **mixture of factor analyzers** since each $p(y, x \\mid z, \\Theta)$ is a factor analysis model. We also recognize it as an analogue of the switching linear dynamical system without any temporal dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6FsPCGYD59F"
      },
      "source": [
        "### Make a Linear Regression Distribution object\n",
        "\n",
        "We'll be using PyTorch Distributions for this lab. PyTorch doesn't include conditional distributions like $p(y \\mid x)$, so we've written a lightweight object to encapsulate the parameters of the linear Gaussian observation model as well. We call it an `IndependentLinearRegression` because the observation covariance $R$ is a diagonal matrix, which implies independent noise across each output dimension.  This is similar to what you wrote in [Lab 6](06_arhmm.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDSxDTLiSLGU"
      },
      "outputs": [],
      "source": [
        "class IndependentLinearRegression(object):\n",
        "    \"\"\"\n",
        "    An object that encapsulates the weights and covariance of a linear\n",
        "    regression. It has an interface similar to that of PyTorch Distributions.\n",
        "    \"\"\"\n",
        "    def __init__(self, weights, bias, diag_covariance):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        weights: N x D tensor of regression weights\n",
        "        bias: N tensor of regression bias\n",
        "        diag_covariance: N tensor of non-negative variances\n",
        "        \"\"\"\n",
        "        self.data_dim, self.covariate_dim = weights.shape[-2:]\n",
        "        assert bias.shape[-1] == self.data_dim\n",
        "        assert diag_covariance.shape[-1] == self.data_dim\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.diag_covariance = diag_covariance\n",
        "\n",
        "    def log_prob(self, data, covariates):\n",
        "        \"\"\"\n",
        "        Compute the log probability of the data given the covariates using the\n",
        "        model parameters. Note that this function's signature is slightly\n",
        "        different from what you implemented in Lab 7.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: a tensor with lagging dimension $N$, the dimension of the data.\n",
        "        covariates: a tensor with lagging dimension $D$, the covariate dimension\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        lp: a tensor of log likelihoods for each data point and covariate pair.\n",
        "        \"\"\"\n",
        "        predictions = torch.einsum('...d,nd->...n', covariates, self.weights)\n",
        "        predictions += self.bias\n",
        "        lkhd = Normal(predictions, torch.sqrt(self.diag_covariance))\n",
        "        return lkhd.log_prob(data).sum(axis=-1)\n",
        "\n",
        "    def sample(self, covariates):\n",
        "        \"\"\"\n",
        "        Sample data points given covariates.\n",
        "        \"\"\"\n",
        "        predictions = torch.einsum('...d,nd->...n', covariates, self.weights)\n",
        "        predictions += self.bias\n",
        "        lkhd = Normal(predictions, torch.sqrt(self.diag_covariance))\n",
        "        return lkhd.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAdg8J2fSDEw"
      },
      "source": [
        "### Make a mixture of factor analyzers object\n",
        "\n",
        "To get you started, we've written a `MixtureOfFactorAnalyzers` object that encapsulates the generative model. It's built out of `torch.distributions.Distribution` objects, which represent the distributions in the generative model. You're already familiar with the `MultivariateNormal` distribution object, which we will use to represent both $p(x \\mid z)$. We also use the `Categorical` distribution object to represent $p(z)$. We'll take advantage of the distribution objects' broadcasting capability to combine all the conditional distributions $p(x \\mid z=k)$ into one object by using a batch of means and covariances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijbVXU12nYXh"
      },
      "outputs": [],
      "source": [
        "class MixtureOfFactorAnalyzers(object):\n",
        "    def __init__(self, num_states, latent_dim, data_dim, scale=1):\n",
        "        self.num_states = num_states\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        # Initialize the discrete state prior p(z)\n",
        "        self.p_z = Categorical(logits=torch.zeros(num_states))\n",
        "\n",
        "        # Initialize the conditional distributions p(x | z)\n",
        "        self.p_x = MultivariateNormal(\n",
        "            scale * torch.randn(num_states, latent_dim),\n",
        "            torch.eye(latent_dim).repeat(num_states, 1, 1))\n",
        "\n",
        "        # Initialize the observation model p(y | x)\n",
        "        self.p_y = IndependentLinearRegression(\n",
        "            torch.randn(data_dim, latent_dim),\n",
        "            torch.randn(data_dim),\n",
        "            torch.ones(data_dim)\n",
        "        )\n",
        "\n",
        "    # Write property to get the parameters from the underlying objects\n",
        "    # These variable names correspond to the math above.\n",
        "    @property\n",
        "    def pi(self):\n",
        "        return self.p_z.probs\n",
        "\n",
        "    @property\n",
        "    def log_pi(self):\n",
        "        return self.p_z.logits\n",
        "\n",
        "    @property\n",
        "    def bs(self):\n",
        "        return self.p_x.mean\n",
        "\n",
        "    @property\n",
        "    def Qs(self):\n",
        "        return self.p_x.covariance_matrix\n",
        "\n",
        "    @property\n",
        "    def Js(self):\n",
        "        return self.p_x.precision_matrix\n",
        "\n",
        "    @property\n",
        "    def hs(self):\n",
        "        # linear natural paramter h = Q^{-1} b = J b\n",
        "        return torch.einsum('kij,kj->ki', self.Js, self.bs)\n",
        "\n",
        "    @property\n",
        "    def C(self):\n",
        "        return self.p_y.weights\n",
        "\n",
        "    @property\n",
        "    def d(self):\n",
        "        return self.p_y.bias\n",
        "\n",
        "    @property\n",
        "    def R_diag(self):\n",
        "        return self.p_y.diag_covariance\n",
        "\n",
        "    def sample(self, sample_shape=(100,)):\n",
        "        \"\"\"\n",
        "        Draw a sample of the latent variables and data under the MFA model.\n",
        "        \"\"\"\n",
        "        z = self.p_z.sample(sample_shape)\n",
        "        x = MultivariateNormal(self.bs[z], self.Qs[z]).sample()\n",
        "        y = self.p_y.sample(x)\n",
        "        return dict(z=z, x=x, y=y)\n",
        "\n",
        "    def plot(self, data, spc=10):\n",
        "        # Unpack the arguments\n",
        "        z, x, y = data['z'], data['x'], data['y']\n",
        "        K = self.num_states\n",
        "        N = self.data_dim\n",
        "        T = len(y)\n",
        "\n",
        "        # Plot the data\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        for k in range(K):\n",
        "            plt.plot(x[z == k, 0], x[z == k, 1], 'o', color=palette[k], mec='k')\n",
        "        plt.xlabel(\"continuous latente dim 0\")\n",
        "        plt.ylabel(\"continuous latente dim 1\")\n",
        "\n",
        "        # Sort the data by their discrete states for nicer visualization\n",
        "        perm = torch.argsort(z)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(z[perm][None, :], extent=(0, T, -spc, spc * (N + 1)),\n",
        "                   aspect=\"auto\", cmap=cmap, vmin=0, vmax=len(palette)-1,\n",
        "                   alpha=0.5)\n",
        "        plt.plot(y[perm] + spc * torch.arange(N), 'wo', mec='k')\n",
        "        for n in range(N):\n",
        "            plt.plot([0, T], [spc * n, spc * n], ':k')\n",
        "\n",
        "        plt.xlim(0, T)\n",
        "        plt.xlabel(\"data index [sorted by discrete state]\")\n",
        "        plt.ylim(-spc, spc * (N + 1))\n",
        "        plt.yticks(spc * torch.arange(N), range(N))\n",
        "        plt.ylabel(\"data dimension\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP5MOoa6GI1b"
      },
      "source": [
        "### Sample data from the generative model\n",
        "\n",
        "Now we will sample small training and testing datasets from an MFA model with random parameters. We plot the data in two ways: as points in the continuous latent space color coded by discrete label, and then as points in the data space. Don't be fooled by the ordering of the second plot: the samples are arbitrarily ordered, but we've permuted their order to see how different states give rise to better see the distribution corresponding to each discrete state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUzFqgP0ncio"
      },
      "outputs": [],
      "source": [
        "# Construct a model instance.\n",
        "# The scale keyword determines how separated the clusters are in the continuous\n",
        "# latent space.\n",
        "torch.manual_seed(0)\n",
        "num_states = 7\n",
        "latent_dim = 2\n",
        "data_dim = 10\n",
        "model = MixtureOfFactorAnalyzers(num_states, latent_dim, data_dim, scale=3)\n",
        "\n",
        "# Sample from the model\n",
        "num_data = 1000\n",
        "train_data = model.sample(sample_shape=(num_data,))\n",
        "test_data = model.sample(sample_shape=(num_data,))\n",
        "\n",
        "# Plot the data\n",
        "model.plot(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6iEEwHUj7tF"
      },
      "source": [
        "## Part 1: Coordinate Ascent Variational Inference (CAVI)\n",
        "\n",
        "First, we'll implement coordinate ascent variational inference (CAVI) for the mixture of factor analyzers model. We'll use a mean field posterior approximation\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(z, x \\mid y, \\Theta) \\approx \\prod_{t=1}^T q(z_t) \\, q(x_t)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "such that $\\mathrm{KL}\\big( q(z)q(x) \\, \\| \\, p(z, x \\mid y, \\Theta) \\big)$ is minimized. In class, we showed how to minimize the KL via coordinate ascent, iteratively optimizing $q(z)$ and $q(x)$, holding the other fixed. Here we will implement that algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkIjezggl8wy"
      },
      "source": [
        "### Problem 1a [Math]: Derive the expected log likelihood\n",
        "\n",
        "In class we derived the coordinate update for the discrete state factors,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log q(z_t) &= \\mathbb{E}_{q(x_t)} \\left[\\log p(z_t, x_t, y \\mid \\Theta) \\right] + \\mathrm{c} \\\\\n",
        "&= \\mathbb{E}_{q(x_t)} \\left[\\log p(z_t \\mid \\Theta) + \\log p(x_t \\mid z_t, \\Theta) + \\log p(y \\mid x_t, \\Theta) \\right] + \\mathrm{c} \\\\\n",
        "&= \\log \\mathrm{Cat}(z_t \\mid \\pi) + \\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_{z_t}, Q_{z_t}) \\right] + \\mathrm{c} \\\\\n",
        "&= \\sum_{k=1}^K \\mathbb{I}[z_t=k] \\left( \\log \\pi_k + \\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_k, Q_k) \\right] \\right) + \\mathrm{c} \\\\\n",
        "&= \\log \\mathrm{Cat}(z_t \\mid \\tilde{\\pi}_t)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log \\tilde{\\pi}_{tk} = \\log \\pi_k + \\underbrace{\\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_k, Q_k) \\right]}_{\\text{expected log likelihood}} + \\mathrm{c}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "However, we did not simplify the expected log likelihood expression.\n",
        "\n",
        "Suppose $q(x_t) = \\mathcal{N}(x_t \\mid \\tilde{\\mu}_t, \\tilde{\\Sigma}_t)$. Show that the expected log likelihood is,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{q(x_t)} \\left[\\log \\mathcal{N}(x_t \\mid b_k, Q_k) \\right]\n",
        "&= \\log \\mathcal{N}(\\tilde{\\mu}_t \\mid b_k, Q_k) - \\tfrac{1}{2} \\langle(Q_k^{-1}, \\tilde{\\Sigma}_t \\rangle\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvbdeHbcRLtO"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjCRNLAAoWWL"
      },
      "source": [
        "### Problem 1b: Implement the discrete state update\n",
        "\n",
        "We will use `torch.distributions.Distribution` objects to represent the  approximate posterior distributions as well. We will use `MultivariateNormal` to represent $q(x)$ and `Categorical` to represent $q(z)$. We'll take advantage of the distribution objects' broadcasting capability to represent the variational posteriors for all time steps at once.\n",
        "\n",
        "_Implement a CAVI update for the discrete states posterior that takes in the model and the continuous state posterior `q_x` and outputs the optimal `q_z`._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SphU492LomR0"
      },
      "outputs": [],
      "source": [
        "def cavi_update_q_z(model, q_x):\n",
        "    \"\"\"Compute the optimal discrete state posterior given the generative model\n",
        "    and the variational posterior on the continuous states.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: a MixtureOfFactorAnalyzers model instance.\n",
        "\n",
        "    q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` and a\n",
        "        shape `TxDxD` parameter `covariance matrix` representing the means and\n",
        "        covariances, respectively, for each data point under the variational\n",
        "        posterior.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    q_z: a `Categorical` object with a shape `TxK` parameter `logits`\n",
        "        representing the variational posterior on discrete states.\n",
        "    \"\"\"\n",
        "    K = model.num_states\n",
        "    T = q_x.mean.shape[0]\n",
        "\n",
        "    logits = torch.zeros(T, K)\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    log_pi = torch.log(model.pi)\n",
        "\n",
        "    for k, (bk, Qk, Jk) in enumerate(zip(model.bs, model.Qs, model.Js)):\n",
        "\n",
        "        diff  = q_x.mean - bk\n",
        "\n",
        "        maha  = torch.sum((diff @ Qk) * diff, dim = 1)\n",
        "        trace = torch.einsum('tij,ji->t', q_x.covariance_matrix, Qk)\n",
        "        logdet = torch.slogdet(Qk)[1]\n",
        "\n",
        "        logits[:, k] = (log_pi[k] + 0.5 * logdet - 0.5 * (maha + trace))\n",
        "\n",
        "    ###\n",
        "    return Categorical(logits=logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYnJYQoN_JlE"
      },
      "outputs": [],
      "source": [
        "def test_1b():\n",
        "    torch.manual_seed(0)\n",
        "    q_x = MultivariateNormal(torch.randn(num_data, latent_dim),\n",
        "                             torch.eye(latent_dim).repeat(num_data, 1, 1))\n",
        "    q_z = cavi_update_q_z(model, q_x)\n",
        "    assert q_z.probs.shape == (num_data, num_states)\n",
        "    assert torch.isclose(q_z.probs.std(), torch.tensor(0.2576), atol=1e-4)\n",
        "test_1b()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiOE-pwKsT3t"
      },
      "source": [
        "### Problem 1c: Implement the continuous state update\n",
        "\n",
        "In class we showed that the optimal continuous state posterior, holding the discrete posterior fixed, was a Gaussian distribution $q(x_t) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\Sigma}_t)$ with\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\tilde{\\mu}_t &= \\tilde{J}_t^{-1} \\tilde{h}_t &\n",
        "\\tilde{\\Sigma}_t &= \\tilde{J}_t^{-1} \\\\\n",
        "\\tilde{h}_t &= \\mathbb{E}_{q({z_t})}[Q_{z_t}^{-1} b_{z_t}] + C^\\top R^{-1} (y_t-d) &\n",
        "\\tilde{J}_t &= \\mathbb{E}_{q({z_t})}[Q_{z_t}^{-1}] + C^\\top R^{-1} C \\\\\n",
        "&= \\sum_{k=1}^K \\left[ q({z_t}=k) Q_k^{-1} b_k \\right] + C^\\top R^{-1} (y_t-d) &\n",
        "&= \\sum_{k=1}^K \\left[ q({z_t}=k) Q_k^{-1} \\right] + C^\\top R^{-1} C\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "_Implement a CAVI update for the continuous states posterior that takes in `p_x`, `p_y`, and `q_z` and outputs the optimal `q_x`._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGDy9EZAtMwS"
      },
      "outputs": [],
      "source": [
        "def cavi_update_q_x(data, model, q_z):\n",
        "    \"\"\"Compute the optimal discrete state posterior given the generative model\n",
        "    and the variational posterior on the continuous states.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: a dictionary with a key `y` containing a `TxN` tensor of data.\n",
        "\n",
        "    model: a MixtureOfFactorAnalyzers model instance.\n",
        "\n",
        "    q_z: a `Categorical` object with a shape `TxK` parameters `logits` and\n",
        "        `probs` representing the variational posterior on discrete states.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean` and a\n",
        "        shape `TxDxD` parameter `covariance matrix` representing the means and\n",
        "        covariances, respectively, for each data point under the variational\n",
        "        posterior.\n",
        "    \"\"\"\n",
        "    y = data[\"y\"]\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    T, N = y.shape\n",
        "    K = model.num_states\n",
        "    D = model.bs[0].shape[0]        # dimensionality of x\n",
        "\n",
        "    # print(dir(model))\n",
        "\n",
        "    # 1) build mixture‐weighted precision J and weighted “force” h\n",
        "    J = torch.zeros(T, D, D, device=y.device)\n",
        "    h = torch.zeros(T, D,    device=y.device)\n",
        "    for k in range(K):\n",
        "        Qk = model.Qs[k]           # (D, D)  precision of p(x|z=k)\n",
        "        bk = model.bs[k]           # (D,)    mean of   p(x|z=k)\n",
        "        w  = q_z.probs[:, k]       # (T,)\n",
        "        J += w[:, None, None] * Qk\n",
        "        h += w[:, None]        * (Qk @ bk)\n",
        "\n",
        "    # 2) add the observation terms C^T R^{-1} C and C^T R^{-1}(y-d)\n",
        "    Rinv = torch.diag(1.0 / model.R_diag)   # now Rinv is (N,N)\n",
        "    J    = J + model.C.T @ Rinv @ model.C\n",
        "    h    = h + (y - model.d) @ Rinv @ model.C\n",
        "\n",
        "    # 3) invert J to get Σ and then μ = Σ h\n",
        "    Sigma = torch.linalg.inv(J)                         # (T, D, D)\n",
        "    mu    = torch.einsum('tij,tj->ti', Sigma, h)        # (T, D)\n",
        "\n",
        "    q_x = MultivariateNormal(loc=mu, covariance_matrix=Sigma)\n",
        "\n",
        "    ###\n",
        "    return q_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osDlPzu8DpTT"
      },
      "outputs": [],
      "source": [
        "def test_1c():\n",
        "    torch.manual_seed(0)\n",
        "    q_z = Categorical(logits=torch.randn(num_data, num_states))\n",
        "    q_x = cavi_update_q_x(train_data, model, q_z)\n",
        "    assert q_x.mean.shape == (num_data, latent_dim)\n",
        "    assert q_x.covariance_matrix.shape == (num_data, latent_dim, latent_dim)\n",
        "    assert torch.isclose(q_x.mean.mean(), torch.tensor(-0.7204), atol=1e-4)\n",
        "    assert torch.isclose(q_x.mean.std(), torch.tensor(2.9253), atol=1e-4)\n",
        "    assert torch.isclose(q_x.covariance_matrix.mean(), torch.tensor(0.0271), atol=1e-4)\n",
        "    assert torch.isclose(q_x.covariance_matrix.std(), torch.tensor(0.0623), atol=1e-4)\n",
        "test_1c()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl8jKsjtH8mu"
      },
      "source": [
        "### Problem 1d [Short Answer]: Intuition for the continuous updates\n",
        "\n",
        "Consider setting the discrete posterior $q(z)$ to be uniform over the $K$ states and then performing one update of the continuous states. The plot below shows the true values of $x$ and $z$ as color coded dots in 2D, and then it shows the means of the continuous state posterior $q(x)$ found using one step of CAVI. We see that the means of the continuous state posterior are all pulled toward the center. Why would you expect that to happen?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8fj2mwxRLtP"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwWuyglQERCD"
      },
      "outputs": [],
      "source": [
        "def plot_data_and_q_x(data, q_x):\n",
        "    x, z, y = data['x'], data['z'], data['y']\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    for k in range(model.num_states):\n",
        "        plt.plot(x[z == k, 0], x[z == k, 1], 'o', color=palette[k], mec='k')\n",
        "        plt.plot(q_x.mean[z == k, 0], q_x.mean[z == k, 1], 'o', color=palette[k], mfc='none', mec='r', ms=8)\n",
        "    plt.xlabel(\"continuous latente dim 0\")\n",
        "    plt.ylabel(\"continuous latente dim 1\")\n",
        "\n",
        "q_z = Categorical(logits=torch.zeros(num_data, model.num_states))\n",
        "q_x = cavi_update_q_x(train_data, model, q_z)\n",
        "plot_data_and_q_x(train_data, q_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nero-ljVw37-"
      },
      "source": [
        "### Problem 1e [Math]: Derive the evidence lower bound\n",
        "\n",
        "We will use the ELBO to track the convergence of our CAVI algorithm. In class we wrote the ELBO as,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(q, \\Theta) &= \\mathbb{E}_{q(z)q(x)} \\left[ \\log p(z, x, y \\mid \\Theta) - \\log q(z)q(x) \\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Show that this is equivalent to,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(q, \\Theta)\n",
        "&= \\mathbb{E}_{q(x)}\\left[\\log p(y \\mid x, \\Theta) \\right]\n",
        "- \\mathrm{KL}\\big(q(z) \\, \\| \\, p(z \\mid \\Theta) \\big) - \\mathbb{E}_{q(z)}\\left[\\mathrm{KL}\\big( q(x) \\, \\| \\, p(x \\mid z, \\Theta) \\big) \\right].\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Then show that,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{q(x)}\\left[\\log p(y \\mid x, \\Theta) \\right]\n",
        "&= \\sum_{t=1}^T \\log \\mathcal{N}(y_t \\mid C \\tilde{\\mu}_t + d, R) -\\tfrac{1}{2} \\langle C^\\top R^{-1} C, \\tilde{\\Sigma}_t \\rangle,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\tilde{\\mu}_t$ and $\\tilde{\\Sigma}_t$ are the parameters of the variational posterior $q(x_t)$, as above.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VggLHjwyRLtP"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQyXC1CnzpHo"
      },
      "source": [
        "### Problem 1f: Implement the ELBO\n",
        "\n",
        "Use the `IndependentLinearRegression.log_prob` function and the `torch.distributions.kl_divergence` function imported at the top of the notebook to implement the ELBO calculation. Remember that the log probabilities and KL divergence functions broadcast nicely."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "fklM5DzR8qUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVeuU5Mvzz6p"
      },
      "outputs": [],
      "source": [
        "def elbo(data, model, variational_posterior):\n",
        "    \"\"\"Compute the optimal discrete state posterior given the generative model\n",
        "    and the variational posterior on the continuous states.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: a dictionary with a key `y` containing a `TxN` tensor of data.\n",
        "\n",
        "    model: a MixtureOfFactorAnalyzers model instance\n",
        "\n",
        "    variational_posterior: a tuple (q_z, q_x) where\n",
        "        q_z: a `Categorical` object with a shape `TxK` parameter `logits`\n",
        "            representing the variational posterior on discrete states.\n",
        "\n",
        "        q_x: a `MultivariateNormal` object with a shape `TxD` parameter `mean`\n",
        "            and a shape `TxDxD` parameter `covariance matrix` representing the\n",
        "            means and covariances, respectively, for each data point under the\n",
        "            variational posterior.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    The evidence lower bound (ELBO) as derived above.\n",
        "    \"\"\"\n",
        "    y = data[\"y\"]\n",
        "    q_z, q_x = variational_posterior\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1.  E_q(x)[log p(y|x,Θ)]\n",
        "    # ------------------------------------------------------------------\n",
        "    T, N        = y.shape\n",
        "    D           = q_x.mean.shape[1]\n",
        "\n",
        "    # Predicted mean of y under q(x)\n",
        "    y_hat       = q_x.mean @ model.C.T + model.d          # (T, N)\n",
        "    resid       = y - y_hat                               # (T, N)\n",
        "\n",
        "    R_diag      = model.R_diag                            # (N,)\n",
        "    logdet_R    = torch.sum(torch.log(R_diag))            # scalar\n",
        "\n",
        "    const       = -0.5 * (N * math.log(2 * math.pi) + logdet_R)\n",
        "    ll_mean     = const - 0.5 * torch.sum(resid**2 / R_diag, dim=1)  # (T,)\n",
        "\n",
        "    # Trace term  -½⟨CᵀR⁻¹C , Σ̃_t⟩\n",
        "    R_inv       = torch.diag(1.0 / R_diag)                # (N, N)\n",
        "    G           = model.C.T @ (R_inv @ model.C)           # (D, D)\n",
        "    trace_term  = -0.5 * torch.einsum(\"ij,tij->t\", G, q_x.covariance_matrix)\n",
        "\n",
        "    expected_log_like = (ll_mean + trace_term).sum()      # scalar\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2.  − KL(q(z) || p(z|Θ))\n",
        "    # ------------------------------------------------------------------\n",
        "    p_z         = Categorical(probs=model.pi)             # (K,)\n",
        "    kl_z        = kl_divergence(q_z, p_z).sum()           # scalar\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3.  − E_q(z)[KL(q(x) || p(x|z,Θ))]\n",
        "    # ------------------------------------------------------------------\n",
        "    kl_x_all = []\n",
        "    for k in range(model.num_states):\n",
        "        p_x_k  = MultivariateNormal(loc=model.bs[k],\n",
        "                                    precision_matrix=model.Qs[k])\n",
        "        kl_k   = kl_divergence(q_x, p_x_k)                # (T,)\n",
        "        kl_x_all.append(kl_k)\n",
        "    kl_x_all   = torch.stack(kl_x_all, dim=1)             # (T, K)\n",
        "    kl_x       = (q_z.probs * kl_x_all).sum()             # scalar\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    elbo = expected_log_like - kl_z - kl_x\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    ###\n",
        "    return elbo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-iRqZgZpL4S"
      },
      "outputs": [],
      "source": [
        "def test_1f():\n",
        "    q_z = Categorical(logits=torch.zeros(num_data, model.num_states))\n",
        "    q_x = cavi_update_q_x(train_data, model, q_z)\n",
        "    assert torch.isclose(elbo(train_data, model, (q_z, q_x)) / num_data,\n",
        "                         torch.tensor(-32.3214), atol=1e-4)\n",
        "\n",
        "test_1f()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Fq6PbKLUW2"
      },
      "source": [
        "### Problem 1g [Math]: Derive the exact marginal likelihood\n",
        "\n",
        "In this simple model, we can actually compute the marginal likelihood exactly. This gives us a way of seeing how tight the ELBO actually is. (Remember, the ELBO is a lower bound on the marginal likelihood!)\n",
        "\n",
        "To compute the marginal likelihood, we need two key facts about Gaussian random variables:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "x \\sim \\mathcal{N}(b, Q) &\\implies C x + d \\sim \\mathcal{N}(Cb + d, CQ C^\\top) \\\\\n",
        "m \\sim \\mathcal{N}(\\mu_1, \\Sigma_1),\n",
        "\\epsilon \\sim \\mathcal{N}(\\mu_2, \\Sigma_2) &\\implies\n",
        "m + \\epsilon \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\Sigma_1 + \\Sigma_2)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Use these two facts to show that\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(y_t \\mid z_t, \\Theta) &= \\mathcal{N}(C b_{z_t} + d, C Q_{z_t} C^\\top + R).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Then show that\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log p(y \\mid \\Theta) &= \\sum_{t=1}^T \\log \\left( \\sum_{k=1}^K \\pi_{k} \\, \\mathcal{N}(y_t \\mid C b_{k} + d, C Q_{k} C^\\top + R) \\right).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNCTcL_6RLtQ"
      },
      "source": [
        "_Your answer here_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K8IkfD9OVES"
      },
      "source": [
        "### Implement the exact marginal likelihood\n",
        "The code below implements the exact marginal likelihood according to the formula above using PyTorch's `LowRankMultivariateNormal` distribution. Note: this distribution takes in the square root of $C Q_k C^\\top$, which is $C Q_k^{1/2}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO2Eoe8qOnYc"
      },
      "outputs": [],
      "source": [
        "def exact_marginal_lkhd(data, model):\n",
        "    \"\"\"\n",
        "    Compute the exact marginal likelihood.\n",
        "    Normalize by the number of datapoints.\n",
        "    \"\"\"\n",
        "    # Compute the marginal distributions\n",
        "    y = data[\"y\"]\n",
        "    T = y.shape[0]\n",
        "    K = model.num_states\n",
        "\n",
        "    # Compute the marginal likelihood under each discrete state assignment\n",
        "    lls = torch.zeros(T, K)\n",
        "    for k, (bk, Qk) in enumerate(zip(model.bs, model.Qs)):\n",
        "        # log p(z = k)\n",
        "        lls[:, k] += model.log_pi[k]\n",
        "\n",
        "        # logp(y | z = k) = log N(y | C b_k + d, C Q_k C^T + diag(R))\n",
        "        Qk_sqrt = torch.linalg.cholesky(Qk)\n",
        "        p_yk = LowRankMultivariateNormal(model.C @ bk + model.d,\n",
        "                                         model.C @ Qk_sqrt, model.R_diag)\n",
        "        lls[:, k] += p_yk.log_prob(y)\n",
        "\n",
        "    return torch.logsumexp(lls, axis=1).sum()\n",
        "\n",
        "marginal_ll = exact_marginal_lkhd(train_data, model) / num_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_7XiTJYHAAu"
      },
      "source": [
        "### Run CAVI\n",
        "\n",
        "That's all we need for CAVI! The code below simply alternates between updating $q(z)$ and $q(x)$. After each iteration, we compute the ELBO.We allow the user to pass in an initial posterior approximation (though only $q(z)$ is used since $q(x)$ is immediately updated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1kpsIk9z2YS"
      },
      "outputs": [],
      "source": [
        "def cavi(data, model, initial_posterior=None, num_steps=10, pbar=None):\n",
        "    y = data[\"y\"]\n",
        "\n",
        "    # Initialize the discrete state posterior to uniform\n",
        "    if initial_posterior is None:\n",
        "        q_z = Categorical(logits=torch.zeros(len(y), model.num_states))\n",
        "        q_x = None\n",
        "    else:\n",
        "        q_z, _ = initial_posterior\n",
        "\n",
        "    # Optional progress bar\n",
        "    if pbar is not None: pbar.reset()\n",
        "\n",
        "    # Run CAVI\n",
        "    avg_elbos = []\n",
        "    for i in range(num_steps):\n",
        "        if pbar is not None: pbar.update()\n",
        "        q_x = cavi_update_q_x(data, model, q_z)\n",
        "        avg_elbos.append(elbo(data, model, (q_z, q_x)) / len(y))\n",
        "        q_z = cavi_update_q_z(model, q_x)\n",
        "\n",
        "    return torch.tensor(avg_elbos), (q_z, q_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7257IHdul92"
      },
      "outputs": [],
      "source": [
        "# Run CAVI and plot the ELBO over coordinate ascent iterations\n",
        "avg_elbos, (q_z, q_x) = cavi(train_data, model)\n",
        "plot_elbos(avg_elbos, marginal_ll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mQnkN7AMBDN"
      },
      "source": [
        "### Re-examine the continuous state posterior after CAVI\n",
        "\n",
        "Now let's make the same plot from Problem 1d again. We should see that the continuous means are pulled toward their true values. Remember, these are inferences! The CAVI algorithm only sees the data $y$ and the model parameters $\\Theta$. After a few iterations (really, after about 2 iterations), it converges to a posterior approximation in which the mean of continuous latent states, $\\mathbb{E}_{q(x_t)}[x_t]$, are close to their true values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHoFi1NwH0Lc"
      },
      "outputs": [],
      "source": [
        "plot_data_and_q_x(train_data, q_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FURED-xnwUDz"
      },
      "source": [
        "## Part 2: Variational EM in a mixture of factor analysis models\n",
        "\n",
        "The CAVI algorithm we implemented in Part 1 will form the E step for variational EM. To complete the algorithm, we just need to compute the expected sufficient statistics under the variational posterior and use them to implement the M-step. Last week, in Lab 7, we derived the expected sufficient statistics needed to update the multivariate normal distribution and the weights of the linear regression. In this part, you'll write similar functions to compute the expected sufficient statistics using the variational posterior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM192bwhdD2J"
      },
      "source": [
        "### Problem 2a: Compute the expected sufficient statistics\n",
        "\n",
        "\n",
        "The sufficient statistics of the model are (with zero-indexing for Python friendliness),\n",
        "\n",
        "0. $\\sum_{t=1}^T \\mathbb{I}[z_t=k]$ for $k = 1, \\ldots, K$\n",
        "1. $\\sum_{t=1}^T \\mathbb{I}[z_t=k] \\, x_t$ for $k = 1, \\ldots, K$\n",
        "2. $\\sum_{t=1}^T \\mathbb{I}[z_t=k] \\, x_t x_t^\\top$ for $k = 1, \\ldots, K$\n",
        "3.  $\\sum_{t=1}^T x_t$\n",
        "4. $\\sum_{t=1}^T x_t x_t^\\top$\n",
        "5. $\\sum_{t=1}^T y_t x_t^\\top$\n",
        "6. $\\sum_{t=1}^T y_t$\n",
        "7. $\\sum_{t=1}^T y_t^2$\n",
        "7. $\\sum_{t=1}^T 1 = T$\n",
        "\n",
        "Write a function that computes the _expected_ sufficient statistics $\\mathbb{E}_{q(z)q(x)}[\\cdot]$ under the variational posterior distribution. In code, we'll call these variables `E_*`, for example `E_z` represents the length $K$ tensor for the sufficient statistic 0.\n",
        "\n",
        "**Note:** The expected outer product, $\\mathbb{E}_{q(x_t)}[x_t x_t^\\top]$, does _not_ equal the covariance matrix unless $\\mathbb{E}_{q(x_t)}[x_t]$ is zero (and here, it's not generally zero).\n",
        "\n",
        "**Note:** Statistics 3 and 1 are redundant, as are 4 and 2. We've split them out anyway, as they are used separately in updating the parameters of `p_x` and `p_y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Uhe6Ve4e0he"
      },
      "outputs": [],
      "source": [
        "def compute_expected_suffstats(data, posterior):\n",
        "    \"\"\"\n",
        "    Compute the expected sufficient statistics of the data\n",
        "    under the variational posterior\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: a dictionary with a key `y` containing a `TxN` tensor of data.\n",
        "    posterior: a tuple (q_z, q_x) representing the variational posterior, as\n",
        "        computed in part 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A tuple of the 9 expected sufficient statistics in the order listed above.\n",
        "    \"\"\"\n",
        "    y = data[\"y\"]\n",
        "    q_z, q_x = posterior\n",
        "\n",
        "    # ---------- expectations under q(x) ----------\n",
        "    mu        = q_x.mean                 # (T, D)\n",
        "    Sigma     = q_x.covariance_matrix    # (T, D, D)\n",
        "    E_xx_full = Sigma + torch.einsum(\"ti,tj->tij\", mu, mu)   # (T, D, D)\n",
        "\n",
        "    # ---------- helper constants ----------\n",
        "    probs     = q_z.probs                # (T, K)\n",
        "    T_val     = y.shape[0]               # scalar (python int)\n",
        "\n",
        "    # ---------- 0.  Σ_t 1[z_t = k] ----------\n",
        "    E_z       = probs.sum(dim=0)                              # (K,)\n",
        "\n",
        "    # ---------- 1.  Σ_t 1[z_t = k] x_t ----------\n",
        "    E_zx      = torch.einsum(\"tk,td->kd\", probs, mu)          # (K, D)\n",
        "\n",
        "    # ---------- 2.  Σ_t 1[z_t = k] x_t x_tᵀ ----------\n",
        "    E_zxxT    = torch.einsum(\"tk,tij->kij\", probs, E_xx_full) # (K, D, D)\n",
        "\n",
        "    # ---------- 3.  Σ_t x_t ----------\n",
        "    E_x       = mu.sum(dim=0)                                 # (D,)\n",
        "\n",
        "    # ---------- 4.  Σ_t x_t x_tᵀ ----------\n",
        "    E_xxT     = E_xx_full.sum(dim=0)                          # (D, D)\n",
        "\n",
        "    # ---------- 5.  Σ_t y_t x_tᵀ ----------\n",
        "    E_yxT     = torch.einsum(\"tn,td->nd\", y, mu)              # (N, D)\n",
        "\n",
        "    # ---------- 6.  Σ_t y_t ----------\n",
        "    E_y       = y.sum(dim=0)                                  # (N,)\n",
        "\n",
        "    # ---------- 7.  Σ_t y_t² ----------\n",
        "    E_ysq     = (y**2).sum(dim=0)                             # (N,)\n",
        "\n",
        "    # ---------- 8.  Σ_t 1  (just T) ----------\n",
        "    T         = y.shape[0]        # ()\n",
        "\n",
        "    return (E_z, E_zx, E_zxxT, E_x, E_xxT, E_yxT, E_y, E_ysq, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdDJ4P4OhSyC"
      },
      "outputs": [],
      "source": [
        "def test_2a():\n",
        "    print(\"This test only checks the shapes, not the values!\")\n",
        "    stats = compute_expected_suffstats(train_data, (q_z, q_x))\n",
        "    assert len(stats) == 9\n",
        "    E_z, E_zx, E_zxxT, E_x, E_xxT, E_yxT, E_y, E_ysq, T = stats\n",
        "    assert E_z.shape == (num_states,)\n",
        "    assert E_zx.shape == (num_states, latent_dim)\n",
        "    assert E_zxxT.shape == (num_states, latent_dim, latent_dim)\n",
        "    assert E_x.shape == (latent_dim,)\n",
        "    assert E_xxT.shape == (latent_dim, latent_dim)\n",
        "    assert E_yxT.shape == (data_dim, latent_dim)\n",
        "    assert E_y.shape == (data_dim,)\n",
        "    assert E_ysq.shape == (data_dim,)\n",
        "    assert isinstance(T, (int, float))\n",
        "\n",
        "test_2a()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRcofx7oHJZ"
      },
      "source": [
        "### Problem 2b: Implement the M-step for the parameters of $p(z \\mid \\Theta)$\n",
        "\n",
        "Write a function to update the prior distribution on discrete states, $p(z \\mid \\Theta)$, using the expected sufficient statistics. This is part of the M-step for variational EM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWOBtncKoLKW"
      },
      "outputs": [],
      "source": [
        "def update_p_z(stats):\n",
        "    \"\"\"\n",
        "    Compute the parameters $\\pi$ of the $p(z \\mid \\Theta)$ and pack them into\n",
        "    a new Categorical distribution object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stats: a tuple of the 9 sufficient statistics computed above\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A new Categorical object for p_z with a length K tensor of cluster\n",
        "        probabilities.\n",
        "    \"\"\"\n",
        "    E_z = stats[0]\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    T   = stats[-1]         # Python int – total number of data points\n",
        "\n",
        "    # Convert T to a tensor on the same device/dtype as E_z for safe division\n",
        "    T   = torch.as_tensor(T, dtype=E_z.dtype, device=E_z.device)\n",
        "\n",
        "    # Maximum-likelihood (or MAP with a flat Dirichlet) estimate of π\n",
        "    pi  = E_z / T           # (K,), guaranteed to sum to 1 because Σ_k E_z[k] = T\n",
        "\n",
        "    # Numerical safety: ensure pi is strictly positive and renormalize\n",
        "    eps = 1e-12\n",
        "    pi  = (pi + eps) / (pi + eps).sum()\n",
        "\n",
        "    p_z = Categorical(probs=pi)\n",
        "\n",
        "    ###\n",
        "    return p_z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NCwj0xVjaxG"
      },
      "source": [
        "### Problem 2c: Implement the M-step for parameters of $p(x \\mid z, \\Theta)$\n",
        "\n",
        "Perform an M-step on the parameters of $p(x \\mid z, \\Theta)$ using the expected sufficient statistics. As before, add a little to the diagonal of the covariance to ensure positive definiteness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxTpGfXHR0iY"
      },
      "outputs": [],
      "source": [
        "def update_p_x(stats):\n",
        "    \"\"\"\n",
        "    Compute the parameters $\\{b_k, Q_k\\}$ of the $p(x \\mid z, \\Theta)$ and pack\n",
        "    them into a new MultivariateNormal distribution object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stats: a tuple of the 9 sufficient statistics computed above\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A new MultivariateNormal object with KxD mean and KxDxD covariance matrix.\n",
        "    \"\"\"\n",
        "    E_z, E_zx, E_zxxT = stats[:3]\n",
        "    K, D = E_zx.shape\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    jitter = 1e-6\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1.  Component weights / counts\n",
        "    # ------------------------------------------------------------------\n",
        "    N_k = E_z.clamp_min(1e-12)             # avoid divide-by-zero\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2.  Means  b_k  =  Σ_t q(z_t=k) x_t   /   Σ_t q(z_t=k)\n",
        "    # ------------------------------------------------------------------\n",
        "    b_k = E_zx / N_k.unsqueeze(-1)         # (K, D)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3.  Covariances\n",
        "    #     Σ_k =  Σ_t q(z_t=k) x_t x_tᵀ  / N_k  −  b_k b_kᵀ\n",
        "    # ------------------------------------------------------------------\n",
        "    E_xx_k   = E_zxxT / N_k.view(K, 1, 1)                    # (K,D,D)\n",
        "    outer_bb = torch.einsum(\"kd,ke->kde\", b_k, b_k)          # (K,D,D)\n",
        "    Sigma_k  = E_xx_k - outer_bb\n",
        "\n",
        "    # add a tiny jitter for positive-definiteness\n",
        "    eye      = torch.eye(D, device=Sigma_k.device, dtype=Sigma_k.dtype)\n",
        "    Sigma_k  = Sigma_k + jitter * eye.expand(K, D, D)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    p_x = MultivariateNormal(loc=b_k, covariance_matrix=Sigma_k)\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    #\n",
        "    ###\n",
        "    return p_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5DW2NirkX2W"
      },
      "source": [
        "### Problem 2d: Implement the M-step for parameters of $p(y \\mid x, \\Theta)$\n",
        "\n",
        "Following Lab 7, let $\\phi_t = (x_t, 1)$ denote the covariates that go into the linear model for data point $y_t$. Specifically,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(y_t \\mid x_t, \\Theta) &= \\mathcal{N}(y_t \\mid W \\phi_t, R),\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $W = (C, d) \\in \\mathbb{R}^{N \\times D+1}$ is an array containing both the weights and the bias of the linear regression model.\n",
        "\n",
        "To update the linear regression, we need the expected sufficient statistics:\n",
        "- The expected outer product of the data and covariates,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{q(x_t)}[ y \\phi_t^\\top] = \\mathbb{E}_{q(x_t)}[ y (x_t, 1)^\\top]\n",
        "= \\begin{bmatrix} \\mathbb{E}_{q(x_t)}[ y x_t^\\top], & y \\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "- The expected outer product of the covariates with themselves,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{q(x_t)}[ \\phi_t \\phi_t^\\top] = \\mathbb{E}_{q(x_t)}[ (x_t, 1) (x_t, 1)^\\top]\n",
        "= \\begin{bmatrix} \\mathbb{E}_{q(x_t)}[ x_t x_t^\\top], &  \\mathbb{E}_{q(x_t)}[ x_t] \\\\\n",
        "\\mathbb{E}_{q(x_t)}[x_t^\\top], & T\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "These are $N \\times (D+1)$ and $(D+1) \\times (D+1)$ tensors, respectively.\n",
        "\n",
        "Since we are assuming a diagonal covariance matrix, we only need $y_{tn}^2$ instead of the full outer product $y_t y_t^\\top$.  As before, add a bit to the diagonal to ensure positive definiteness."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Independent, Normal"
      ],
      "metadata": {
        "id": "n6C11aFk_To_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJYP-MZS_uK8"
      },
      "outputs": [],
      "source": [
        "def update_p_y(stats):\n",
        "    \"\"\"\n",
        "    Compute the linear regression parameters given the expected\n",
        "    sufficient statistics.\n",
        "\n",
        "    Note: add a little bit to the diagonal of each covariance\n",
        "        matrix to ensure that the result is positive definite.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stats: a tuple of the 8 sufficient statistics computed above\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A new IndependentLinearRegression object for p_y\n",
        "    \"\"\"\n",
        "    E_x, E_xxT, E_yxT, E_y, E_ysq, T = stats[3:]\n",
        "    N, D = E_yxT.shape\n",
        "\n",
        "    ###\n",
        "    # Use E_x, E_xxT, E_yxT, E_y, and T to compute the full expected\n",
        "    # sufficient matrices as described above.\n",
        "    #\n",
        "    # YOUR CODE BELOE\n",
        "\n",
        "    # class IndependentLinearRegression:\n",
        "    #   def __init__(self, C, d, R_diag):\n",
        "    #       self.C, self.d, self.R_diag = C, d, R_diag\n",
        "    #       # self._base = Independent(Normal(0., 1.), 1)        # helper for log-prob\n",
        "\n",
        "    jitter = 1e-6\n",
        "\n",
        "    E_x, E_xxT, E_yxT, E_y, E_ysq, T = stats[3:]     # unpack\n",
        "    N, D  = E_yxT.shape\n",
        "    T     = torch.as_tensor(T, dtype=E_x.dtype, device=E_x.device)\n",
        "\n",
        "    # ----- build normal-equation blocks (all in one go) --------------------\n",
        "    S_phi_phi = torch.cat([\n",
        "        torch.cat([E_xxT, E_x.unsqueeze(1)], dim=1),\n",
        "        torch.cat([E_x.unsqueeze(0), T.view(1, 1)], dim=1)   # 👈  key change\n",
        "    ], dim=0)                                             # (D+1, D+1)\n",
        "    S_phi_phi += jitter * torch.eye(D+1, device=E_x.device)\n",
        "\n",
        "\n",
        "    S_y_phi   = torch.cat([E_yxT, E_y[:, None]], dim=1)  # (N, D+1)\n",
        "\n",
        "    # ----- solve for W = [C | d] ---------------------------\n",
        "    #   W = S_yφ · (S_φφ)⁻¹  -> use 'solve' to avoid explicit inverse\n",
        "    W = torch.linalg.solve(S_phi_phi.T, S_y_phi.T).T      # (N, D+1)\n",
        "    C, d = W[:, :D], W[:, D]\n",
        "\n",
        "    # ----- residual variances (all vectorised) -------------\n",
        "    # term1 = E[y²]                     (N,)\n",
        "    term1 = E_ysq\n",
        "    # term2 = 2 * w · S_yφ              (N,)\n",
        "    term2 = 2 * (W * S_y_phi).sum(dim=1)\n",
        "    # term3 = w · S_φφ · w              (N,)\n",
        "    term3 = (W @ S_phi_phi * W).sum(dim=1)\n",
        "\n",
        "    R_diag = (term1 - term2 + term3) / T\n",
        "    R_diag = R_diag.clamp_min(jitter)\n",
        "\n",
        "    p_y = IndependentLinearRegression(C=C, d=d, R_diag=R_diag)\n",
        "\n",
        "\n",
        "    #\n",
        "    ###\n",
        "\n",
        "    return p_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HlbbA5Fnmc0"
      },
      "source": [
        "### Put it all together\n",
        "\n",
        "From here it's smooth sailing! We just iterate between the variational E step, which involves running CAVI for some number of iterations, and then performing an M step using expected sufficient statistics. We'll track the ELBO throughout to monitor convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVsrfOAznntm"
      },
      "outputs": [],
      "source": [
        "def m_step(data, model, posterior):\n",
        "    \"\"\"\n",
        "    Perform an M-step to update the model parameters given the data and the\n",
        "    posterior from the variational E step.\n",
        "    \"\"\"\n",
        "    stats = compute_expected_suffstats(data, posterior)\n",
        "    model.p_z = update_p_z(stats)\n",
        "    model.p_x = update_p_x(stats)\n",
        "    model.p_y = update_p_y(stats)\n",
        "\n",
        "\n",
        "def variational_em(data, model, num_iters=100, num_cavi_steps=1):\n",
        "    \"\"\"\n",
        "    Fit the model parameters via variational EM.\n",
        "    \"\"\"\n",
        "    # Run CAVI\n",
        "    avg_elbos = []\n",
        "    posterior = None\n",
        "    for i in trange(num_iters):\n",
        "        # Variational E step with CAVI\n",
        "        these_elbos, posterior = cavi(data, model, posterior,\n",
        "                                      num_steps=num_cavi_steps)\n",
        "        avg_elbos.extend(these_elbos)\n",
        "\n",
        "        # M-step\n",
        "        m_step(data, model, posterior)\n",
        "\n",
        "    return torch.tensor(avg_elbos), posterior\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq_7Cqk9tDXe"
      },
      "outputs": [],
      "source": [
        "# Fit the synthetic data\n",
        "avg_elbos, posterior = variational_em(train_data, model)\n",
        "plot_elbos(avg_elbos, marginal_ll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_k4BwLcUnGZ"
      },
      "source": [
        "### Problem 2e [Short Answer]: Interpret the results\n",
        "\n",
        "One perhaps counterintuitive aspect of the output is that the ELBO of the fitted model actually exceeds the marginal likeliood of the true model. How can that happen?\n",
        "\n",
        "_Answer below this line_\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTtPjDOVv7H"
      },
      "source": [
        "### Problem 2f: Cross validation\n",
        "\n",
        "Fit the MFA model with variational EM for $D=1,\\ldots, 5$ (inclusive), keeping the number of discrete states fixed to $K=7$. For each model, evaluate the evidence lower bound on the test data, using ten steps of CAVI to approximate the posterior. Then compute the exact marginal likelihood using the true model and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD4crz3MV0zP"
      },
      "outputs": [],
      "source": [
        "test_latent_dims = torch.arange(1, 11)\n",
        "test_elbos = []\n",
        "for d in test_latent_dims:\n",
        "    print(\"Fitting the MFA model with D =\", int(d),\n",
        "          \"dimensional continuous states.\")\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    # ...\n",
        "    test_elbos.append(...)\n",
        "    #\n",
        "    ###\n",
        "\n",
        "# Compute the true marginal likelihood of the test dat\n",
        "true_test_elbo = exact_marginal_lkhd(test_data, model) / num_data\n",
        "\n",
        "# Plot as a function of continuous dimensionality\n",
        "plt.plot(test_latent_dims, test_elbos, '-o')\n",
        "plt.plot(test_latent_dims, true_test_elbo * torch.ones_like(test_latent_dims), ':k')\n",
        "plt.xlabel(\"continuous dimension $D$\")\n",
        "plt.ylabel(\"Test ELBO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqxThbOlY1Lq"
      },
      "source": [
        "### Problem 2g [Short answer]: Interpret the results\n",
        "\n",
        "Would you be surprised to see the fitted models achieve higher ELBOs on test data than the marginal likelihod of the true model? Can you think of any potential concerns with using the ELBO for model comparison; e.g. for selecting the latent state dimension $D$?\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpuB5R_cRLtV"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37rNTRHwwDnp"
      },
      "source": [
        "## Part 3: Apply it to real data\n",
        "\n",
        "Finally, we'll apply the mixture of factor analyzers to calcium imaging data from immobilized worms studied by Kato et al (2015). They also segmented the time series into discrete states based on the neural activity and gave each state a name, using their knowledge of how different neurons correlate with different types of behavior. We'll try to recapitulate some of their results using the MFA model to infer discrete states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeZtzkFqDJhS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget -nc https://github.com/slinderman/ml4nd/raw/refs/heads/main/data/07_vem/kato2015.zip\n",
        "!unzip -n kato2015.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-AtmZbbuoT"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "The data is stored in a dictionary with a few extra keys for the neuron names and the given discrete state labels and human-interpretable state names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJAVmVAmR3tv"
      },
      "outputs": [],
      "source": [
        "# Load the data for a single worm\n",
        "data = load_kato(index=4)\n",
        "\n",
        "# Extract key constants\n",
        "num_frames, num_neurons = data[\"y\"].shape\n",
        "times = torch.arange(num_frames) / data[\"fps\"]\n",
        "\n",
        "print(data.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezyTBYVwb8Sw"
      },
      "source": [
        "### Perform PCA\n",
        "\n",
        "We'll use the principal components for visualization as well as for finding a permutation of the neurons that puts similar neurons, as measured by their loading on the first principal component, near to one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CccRWyoNUrLr"
      },
      "outputs": [],
      "source": [
        "# Perform PCA\n",
        "pca = PCA(20)\n",
        "data[\"pcs\"] = pca.fit_transform(data[\"y\"])\n",
        "neuron_perm = torch.argsort(torch.tensor(pca.components_[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btm5kHV8cB5n"
      },
      "source": [
        "### Plot the data\n",
        "We plot the time series of neural activity on top of the color-coded discrete states given by Kato et al. You should see that the different discrete states correspond to different levels of neural activity across the population of neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBFRdTodSIQl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(data[\"z_kato\"][None, :], extent=(0, times[-1], -1, num_neurons + 2),\n",
        "           alpha=0.5, cmap=cmap, aspect=\"auto\")\n",
        "plt.plot(times, data[\"y\"][:, neuron_perm] + torch.arange(num_neurons), '-k', lw=1)\n",
        "plt.xlabel(\"time[s]\")\n",
        "plt.ylabel(\"neurons\")\n",
        "plt.yticks(torch.arange(num_neurons),\n",
        "           [data[\"neuron_names\"][i] for i in neuron_perm],\n",
        "           fontsize=10)\n",
        "plt.ylim(-1, num_neurons+2)\n",
        "\n",
        "for state_name, color in zip(data[\"state_names\"], palette):\n",
        "    plt.plot([torch.nan], [torch.nan], '-', color=color, lw=4, label=state_name)\n",
        "\n",
        "plt.legend(loc=\"lower right\", ncol=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwY8A0mFclCR"
      },
      "source": [
        "### Plot the PCA trajectories\n",
        "\n",
        "We can also visualize the population activity as a trajectory through PCA space. Here we plot the trajectory in planes spanned by pairs of principal components. We color code the trajectory based on the given discrete states.\n",
        "\n",
        "**Note**: We smoothed the trajectories a bit to make the visualization nicer.\n",
        "\n",
        "**Note**: These differ from the figures in Kato et al (2015) in that they used PCA on the first order differences in neural activity (akin to the \"spikes\" in the calcium trace, even though _C elegans_ doesn't fire action potentials). We found that the first order differences didn't cluster as nicely with the MFA model, so we are working with the calcium traces directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlLZnT-LUnL2"
      },
      "outputs": [],
      "source": [
        "pcs_smooth = gaussian_filter1d(data[\"pcs\"], 1, axis=0)\n",
        "\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10), sharex=True, sharey=True)\n",
        "for i in range(4):\n",
        "    for j in range(i+1, 5):\n",
        "        plot_2d_continuous_states(pcs_smooth, data[\"z_kato\"],\n",
        "                                  ax=axs[j-1, i], inds=(i, j), lw=1)\n",
        "        axs[j-1, i].set_xlabel(\"PC{}\".format(i))\n",
        "        axs[j-1, i].set_ylabel(\"PC{}\".format(j))\n",
        "\n",
        "    for j in range(i):\n",
        "        axs[j, i].set_axis_off()\n",
        "\n",
        "for state_name, color in zip(data[\"state_names\"], palette):\n",
        "    axs[0, -1].plot([torch.nan], [torch.nan], '-',\n",
        "                    color=color, lw=4, label=state_name)\n",
        "axs[0, -1].legend(loc=\"upper right\", ncol=2, )\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYAx_uStztJH"
      },
      "source": [
        "### Problem 3a [Short Answer]: Interpret the PCA trajectories\n",
        "\n",
        "What can you say about the cycle of neural activity in this worm given the PCA trajectories and the state labels provided by Kato et al (2015)?\n",
        "\n",
        "_Answer below this line_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s_fAx73s8OK"
      },
      "source": [
        "### Fit the mixture of factor analyzers\n",
        "\n",
        "Now fit the model. We'll give it twice as many states as Kato et al (2015) did. This often helps avoid some local optima where states are unused. We'll use ten dimensional continuous latents, as they do in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Az0AOgs-mQ"
      },
      "outputs": [],
      "source": [
        "# Fit the worm data\n",
        "torch.manual_seed(0)\n",
        "num_states = 16\n",
        "latent_dim = 10\n",
        "worm_model = MixtureOfFactorAnalyzers(num_states, latent_dim, num_neurons)\n",
        "\n",
        "# Fit the model!\n",
        "avg_elbos, posterior = variational_em(data, worm_model,\n",
        "                                      num_iters=100,\n",
        "                                      num_cavi_steps=1)\n",
        "\n",
        "plot_elbos(avg_elbos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4fIvl2nfRv2"
      },
      "source": [
        "### Compute the overlap between the given and inferred discrete states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1J4My95x27_"
      },
      "outputs": [],
      "source": [
        "# Find the most likely state segmentation\n",
        "q_z, q_x = posterior\n",
        "z_inf = q_z.probs.argmax(axis=1)\n",
        "\n",
        "# compute overlap with the manually labeled states\n",
        "overlap = torch.zeros(8, num_states)\n",
        "for i in range(8):\n",
        "    for j in range(num_states):\n",
        "        overlap[i, j] = torch.sum((data[\"z_kato\"] == i) * (z_inf == j))\n",
        "\n",
        "# normalize since sum given states are used less frequently than others\n",
        "overlap /= overlap.sum(axis=0)\n",
        "\n",
        "# permute the inferred labels for easier visualization\n",
        "z_perm = torch.argsort(torch.argmax(overlap, axis=0))\n",
        "\n",
        "# show the permuted overlap matrix\n",
        "plt.imshow(overlap[:, z_perm])\n",
        "plt.ylabel(\"Kato et al labels\")\n",
        "plt.yticks(torch.arange(8), data[\"state_names\"])\n",
        "plt.xlabel(\"inferred discrete states\")\n",
        "plt.title(\"overlap (column normalized)\")\n",
        "plt.colorbar()\n",
        "\n",
        "# Permute the inferred discrete states per the new ordering\n",
        "z_inf_perm = torch.argsort(z_perm)[z_inf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk8uhvRBfMt_"
      },
      "source": [
        "### Plot the inferred segmentation and the given state labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmFBlgnLx9sg"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(20, 11),\n",
        "                        gridspec_kw=dict(height_ratios=[1, 10]),\n",
        "                        sharex=True)\n",
        "\n",
        "axs[0].imshow(data[\"z_kato\"][None, :],\n",
        "              extent=(0, times[-1], 0, 1),\n",
        "              alpha=0.8, cmap=cmap, aspect=\"auto\")\n",
        "axs[0].set_xticks([])\n",
        "axs[0].set_yticks([])\n",
        "axs[0].set_ylabel(\"$z_{\\mathsf{Kato}}$\")\n",
        "\n",
        "axs[1].imshow(z_inf_perm[None, :], extent=(0, times[-1], -1, num_neurons + 2),\n",
        "              cmap=cmap, alpha=0.8, aspect=\"auto\")\n",
        "axs[1].plot(times, data[\"y\"][:, neuron_perm] + torch.arange(num_neurons),\n",
        "            '-k', lw=1)\n",
        "axs[1].set_xlabel(\"time[s]\")\n",
        "axs[1].set_yticks(torch.arange(num_neurons))\n",
        "axs[1].set_yticklabels([data[\"neuron_names\"][i] for i in neuron_perm],\n",
        "                       fontsize=10)\n",
        "axs[1].set_ylabel(\"neurons\")\n",
        "axs[1].set_ylim(-1, num_neurons+2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNUyZhkZHiwM"
      },
      "outputs": [],
      "source": [
        "x_inf = q_x.mean\n",
        "x_inf_smooth = gaussian_filter1d(x_inf, 2, axis=0)\n",
        "\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10), sharex=True, sharey=True)\n",
        "for i in range(4):\n",
        "    for j in range(i+1, 5):\n",
        "        plot_2d_continuous_states(x_inf_smooth, data[\"z_kato\"],\n",
        "                                  ax=axs[j-1, i], inds=(i, j), lw=1)\n",
        "        axs[j-1, i].set_xlabel(\"PC{}\".format(i))\n",
        "        axs[j-1, i].set_ylabel(\"PC{}\".format(j))\n",
        "\n",
        "    for j in range(i):\n",
        "        axs[j, i].set_axis_off()\n",
        "\n",
        "for state_name, color in zip(data[\"state_names\"], palette):\n",
        "    axs[0, -1].plot([torch.nan], [torch.nan], '-',\n",
        "                    color=color, lw=4, label=state_name)\n",
        "axs[0, -1].legend(loc=\"upper right\", ncol=2, )\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kab9AFnlRLtW"
      },
      "source": [
        "## Author contributions\n",
        "\n",
        "Write a short paragraph describing how each team member contributed to this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeeVih6nRLtW"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxEz4WjoRLtW"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "\n",
        "Download your notebook in .ipynb format and use the following command to convert it to PDF\n",
        "```\n",
        "jupyter nbconvert --to pdf lab7_name.ipynb\n",
        "```\n",
        "If you're using Anaconda for package management, you can install `nbconvert` with\n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "Upload your .pdf file to Gradescope.\n",
        "\n",
        "**Only one submission per team!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "5b28c5bd4ee93d765ebe901023d5522822fb8ad083dac3187c5545022f913719"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}