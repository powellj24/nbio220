{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkInOPFnzoQK"
      },
      "source": [
        "# Generalized Linear Models\n",
        "\n",
        "In this lab, you'll build generalized linear models (GLMs) and convolutional neural network (CNN) models of retinal ganglion cell (RGC) responses to visual stimuli. You'll use PyTorch to implement the models and fit them to a dataset  kindly provided by the [Baccus Lab](https://baccuslab.stanford.edu/) (Stanford University), which they studied in the \"Deep Retina\" paper [(McIntosh et al, 2016)](https://arxiv.org/abs/1702.01825).\n",
        "\n",
        "**References:**\n",
        "\n",
        "McIntosh, Lane T., Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen A. Baccus. “Deep Learning Models of the Retinal Response to Natural Scenes.” Advances in Neural Information Processing (NeurIPS), 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GKupsmKxxvx"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY1dqa8CgQqi",
        "outputId": "557797a3-ccdd-456a-9a76-7930b69f7720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
            "  Downloading wadler_lindig-0.1.5-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.5-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping\n",
            "Successfully installed jaxtyping-0.3.2 wadler-lindig-0.1.5\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import jaxtyping\n",
        "except:\n",
        "    !pip install jaxtyping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jn6yXI4rXhof"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, Dict, Optional\n",
        "from jaxtyping import Float\n",
        "from torch import Tensor\n",
        "from torch.distributions import Poisson\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import trange\n",
        "\n",
        "# Specify that we want our tensors on the GPU and in float32\n",
        "device = torch.device('cuda')\n",
        "dtype = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjI6pKa0gQqj"
      },
      "source": [
        "### Helper functions for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "X0H23MgeD03y",
        "tags": [
          "hide-cell"
        ]
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for plotting (run this cell!)\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "# initialize a color palette for plotting\n",
        "palette = sns.xkcd_palette([\"windows blue\",\n",
        "                            \"red\",\n",
        "                            \"medium green\",\n",
        "                            \"dusty purple\",\n",
        "                            \"orange\",\n",
        "                            \"amber\",\n",
        "                            \"clay\",\n",
        "                            \"pink\",\n",
        "                            \"greyish\"])\n",
        "\n",
        "def plot_stimulus_weights(glm: nn.Module):\n",
        "    \"\"\"\n",
        "    Plot the stimulus weights of a GLM.\n",
        "    \"\"\"\n",
        "    num_neurons = glm.num_neurons\n",
        "    max_delay = glm.max_delay\n",
        "\n",
        "    fig, axs = plt.subplots(num_neurons, 3, figsize=(8, 4 * num_neurons),\n",
        "                            gridspec_kw=dict(width_ratios=[1, 1.9, .1]))\n",
        "\n",
        "    temporal_weights = glm.temporal_conv.weight[:, 0].to(\"cpu\").detach()\n",
        "    bias = glm.temporal_conv.bias.to(\"cpu\").detach()\n",
        "    spatial_weights = glm.spatial_conv.weight.to(\"cpu\").detach()\n",
        "    spatial_weights = spatial_weights.reshape(num_neurons, 50, 50)\n",
        "\n",
        "    # normalize and flip the spatial weights\n",
        "    for n in range(num_neurons):\n",
        "        # Flip if spatial weight peak is negative\n",
        "        if torch.allclose(spatial_weights[n].min(),\n",
        "                       -abs(spatial_weights[n]).max()):\n",
        "            spatial_weights[n] = -spatial_weights[n]\n",
        "            temporal_weights[n] = -temporal_weights[n]\n",
        "\n",
        "        # Normalize\n",
        "        scale = torch.linalg.norm(spatial_weights[n])\n",
        "        spatial_weights[n] /= scale\n",
        "        temporal_weights[n] *= scale\n",
        "\n",
        "    # Set the same limits for each neuron\n",
        "    vlim = abs(spatial_weights).max()\n",
        "    ylim = abs(temporal_weights).max()\n",
        "\n",
        "    for n in range(num_neurons):\n",
        "        axs[n, 0].plot(torch.arange(-max_delay+1, 1) * 10, temporal_weights[n])\n",
        "        axs[n, 0].set_ylim(-ylim, ylim)\n",
        "        axs[n, 0].plot(torch.arange(-max_delay+1, 1) * 10, torch.zeros(max_delay), ':k')\n",
        "        if n < num_neurons - 1:\n",
        "            axs[n, 0].set_xticklabels([])\n",
        "        else:\n",
        "            axs[n, 0].set_xlabel(\"$\\Delta t$ [ms]\")\n",
        "\n",
        "        im = axs[n, 1].imshow(spatial_weights[n],\n",
        "                              vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
        "        axs[n, 1].set_axis_off()\n",
        "        axs[n, 1].set_title(\"neuron {}\".format(n + 1))\n",
        "        plt.colorbar(im, cax=axs[n, 2])\n",
        "\n",
        "\n",
        "def plot_coupling_weights(glm: nn.Module):\n",
        "    \"\"\"\n",
        "    Plot the coupling weights of a GLM.\n",
        "    \"\"\"\n",
        "    # Get the weights and flip them to get time after spike\n",
        "    W = glm.coupling_conv.weight.to(\"cpu\").detach()\n",
        "    W = torch.flip(W, dims=(2,))\n",
        "    num_neurons = W.shape[0]\n",
        "    wlim = abs(W).max()\n",
        "    dt = 10 * torch.arange(W.shape[2])\n",
        "\n",
        "    fig, axs = plt.subplots(num_neurons, num_neurons, figsize=(12, 12),\n",
        "                            sharex=True, sharey=True)\n",
        "    for i in range(num_neurons):\n",
        "        for j in range(num_neurons):\n",
        "            axs[i, j].plot(dt, 0 * dt, ':k')\n",
        "            axs[i, j].plot(dt, W[i, j])\n",
        "            axs[i, j].set_ylim(-wlim, wlim)\n",
        "            axs[i, j].set_title(\"${} \\\\to {}$\".format(j, i))\n",
        "\n",
        "            if i == num_neurons - 1:\n",
        "                axs[i, j].set_xlabel(\"$\\Delta t$ [ms]\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_cnn_subunits_1(cnn: nn.Module):\n",
        "    \"\"\"\n",
        "    Plot the first layer of subunits of a CNN.\n",
        "    \"\"\"\n",
        "    num_subunits = cnn.num_subunits_1\n",
        "    max_delay = cnn.max_delay\n",
        "\n",
        "    fig, axs = plt.subplots(num_subunits, 3, figsize=(8, 4 * num_subunits),\n",
        "                            gridspec_kw=dict(width_ratios=[1, 1.9, .1]))\n",
        "\n",
        "    temporal_weights = cnn.temporal_conv.weight[:, 0].to(\"cpu\").detach()\n",
        "    bias = cnn.temporal_conv.bias.to(\"cpu\").detach()\n",
        "    spatial_weights = cnn.spatial_conv.weight.to(\"cpu\").detach()\n",
        "    spatial_weights = spatial_weights[:, 0, :, :]\n",
        "\n",
        "    # normalize and flip the spatial weights\n",
        "    for n in range(num_subunits):\n",
        "        # Flip if spatial weight peak is negative\n",
        "        if torch.allclose(spatial_weights[n].min(),\n",
        "                    -abs(spatial_weights[n]).max()):\n",
        "            spatial_weights[n] = -spatial_weights[n]\n",
        "            temporal_weights[n] = -temporal_weights[n]\n",
        "\n",
        "        # Normalize\n",
        "        scale = torch.linalg.norm(spatial_weights[n])\n",
        "        spatial_weights[n] /= scale\n",
        "        temporal_weights[n] *= scale\n",
        "\n",
        "    # Set the same limits for each neuron\n",
        "    vlim = abs(spatial_weights).max()\n",
        "    ylim = abs(temporal_weights).max()\n",
        "\n",
        "    for n in range(num_subunits):\n",
        "        axs[n, 0].plot(torch.arange(-max_delay+1, 1) * 10, temporal_weights[n])\n",
        "        axs[n, 0].set_ylim(-ylim, ylim)\n",
        "        axs[n, 0].plot(torch.arange(-max_delay+1, 1) * 10, torch.zeros(max_delay), ':k')\n",
        "        if n < num_subunits - 1:\n",
        "            axs[n, 0].set_xticklabels([])\n",
        "        else:\n",
        "            axs[n, 0].set_xlabel(\"$\\Delta t$ [ms]\")\n",
        "\n",
        "        im = axs[n, 1].imshow(spatial_weights[n],\n",
        "                              vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
        "        axs[n, 1].set_axis_off()\n",
        "        axs[n, 1].set_title(\"subunit 1,{}\".format(n + 1))\n",
        "        plt.colorbar(im, cax=axs[n, 2])\n",
        "\n",
        "def plot_cnn_subunits2(cnn: nn.Module):\n",
        "    \"\"\"\n",
        "    Plot the second layer of subunits of a CNN.\n",
        "    \"\"\"\n",
        "    cnn_filters_2 = cnn.layer2.weight.to(\"cpu\").detach()\n",
        "\n",
        "    fig, axs = plt.subplots(cnn.num_subunits_2,\n",
        "                            cnn.num_subunits_1,\n",
        "                            figsize=(4 * cnn.num_subunits_2,\n",
        "                                    4 * cnn.num_subunits_1),\n",
        "                            sharex=True, sharey=True)\n",
        "    vlim = abs(cnn_filters_2).max()\n",
        "    for i in range(cnn.num_subunits_2):\n",
        "        for j in range(cnn.num_subunits_1):\n",
        "            axs[i, j].imshow(cnn_filters_2[i, j],\n",
        "                            vmin=-vlim, vmax=vlim, cmap=\"RdBu\")\n",
        "\n",
        "            axs[i, j].set_title('subunit 1,{} $\\\\to$ 2,{}'.format(j+1,i+1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXtkdF0DgQqj"
      },
      "source": [
        "### Helper function to train a Pytorch model.\n",
        "\n",
        "We've slightly modified the `train_model` function from the previous lab. This version keeps track of the model with the best validation loss over the course of the training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "z9HtrRrE4ox9",
        "tags": [
          "hide-cell"
        ]
      },
      "outputs": [],
      "source": [
        "#@title Implement `train_model` function (run this cell!)\n",
        "def train_model(model: nn.Module,\n",
        "                train_dataset: Dataset,\n",
        "                val_dataset: Dataset,\n",
        "                objective: callable,\n",
        "                regularizer: Optional[callable]=None,\n",
        "                num_epochs: int=100,\n",
        "                lr: float=0.1,\n",
        "                momentum: float=0.9,\n",
        "                lr_step_size: int=25,\n",
        "                lr_gamma: float=0.9\n",
        "                ) -> Tuple[Float[Tensor, \" num_epochs\"],\n",
        "                           Float[Tensor, \" num_epochs\"]]:\n",
        "    \"\"\"\n",
        "    Train a model on the training dataset and validate it on the validation\n",
        "    dataset. The model is trained using stochastic gradient descent with a\n",
        "    decaying learning rate. The model is trained for `num_epochs` epochs and\n",
        "    the learning rate is decayed every `lr_step_size` epochs by a factor of\n",
        "    `lr_gamma`. The model is trained using the specified `objective` function\n",
        "    and an optional `regularizer`.\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        The model to be trained.\n",
        "    train_dataset : Dataset\n",
        "        The training dataset.\n",
        "    val_dataset : Dataset\n",
        "        The validation dataset.\n",
        "    objective : callable\n",
        "        The objective function to be minimized.\n",
        "    regularizer : callable, optional\n",
        "        The regularizer to be added to the objective function. The default\n",
        "        is None.\n",
        "    num_epochs : int, optional\n",
        "        The number of epochs to train the model. The default is 100.\n",
        "    lr : float, optional\n",
        "        The learning rate for the optimizer. The default is 0.1.\n",
        "    momentum : float, optional\n",
        "        The momentum for the optimizer. The default is 0.9.\n",
        "    lr_step_size : int, optional\n",
        "        The number of epochs after which to decay the learning rate. The\n",
        "        default is 25.\n",
        "    lr_gamma : float, optional\n",
        "        The factor by which to decay the learning rate. The default is 0.9.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Float[Tensor, \" num_epochs\"], Float[Tensor, \" num_epochs\"]]\n",
        "        The training and validation losses for each epoch.\n",
        "    \"\"\"\n",
        "    # progress bars\n",
        "    pbar = trange(num_epochs)\n",
        "    pbar.set_description(\"---\")\n",
        "    inner_pbar = trange(len(train_dataset))\n",
        "    inner_pbar.set_description(\"Batch\")\n",
        "\n",
        "    # data loaders for train and validation\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
        "    dataloaders = dict(train=train_dataloader, val=val_dataloader)\n",
        "\n",
        "    # use standard SGD with a decaying learning rate\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=lr,\n",
        "                          momentum=momentum)\n",
        "    scheduler = lr_scheduler.StepLR(optimizer,\n",
        "                                    step_size=lr_step_size,\n",
        "                                    gamma=lr_gamma)\n",
        "\n",
        "    # Keep track of the best model\n",
        "    best_model_wts = deepcopy(model.state_dict())\n",
        "    best_loss = 1e8\n",
        "\n",
        "    # Track the train and validation loss\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'val']:\n",
        "            # set model to train/validation as appropriate\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "                inner_pbar.reset()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            # track the running loss over batches\n",
        "            running_loss = 0\n",
        "            running_size = 0\n",
        "            for datapoint in dataloaders[phase]:\n",
        "                stim_t = datapoint['stimulus'].squeeze(0)\n",
        "                spikes_t = datapoint['spikes'].squeeze(0)\n",
        "                if phase == \"train\":\n",
        "                    with torch.set_grad_enabled(True):\n",
        "                        optimizer.zero_grad()\n",
        "                        # compute the model output and loss\n",
        "                        output_t = model(stim_t, spikes_t)\n",
        "                        loss_t = objective(output_t, spikes_t)\n",
        "                        # only add the regularizer in the training phase\n",
        "                        if regularizer is not None:\n",
        "                            loss_t += regularizer(model)\n",
        "\n",
        "                        # take the gradient and perform an sgd step\n",
        "                        loss_t.backward()\n",
        "                        optimizer.step()\n",
        "                    inner_pbar.update(1)\n",
        "                else:\n",
        "                    # just compute the loss in validation\n",
        "                    output_t = model(stim_t, spikes_t)\n",
        "                    loss_t = objective(output_t, spikes_t)\n",
        "\n",
        "                assert torch.isfinite(loss_t)\n",
        "                running_loss += loss_t.item()\n",
        "                running_size += 1\n",
        "\n",
        "            # compute the train/validation loss and update the best\n",
        "            # model parameters if this is the lowest validation loss yet\n",
        "            running_loss /= running_size\n",
        "            if phase == \"train\":\n",
        "                train_losses.append(running_loss)\n",
        "            else:\n",
        "                val_losses.append(running_loss)\n",
        "                if running_loss < best_loss:\n",
        "                    best_loss = running_loss\n",
        "                    best_model_wts = deepcopy(model.state_dict())\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update the progress bar\n",
        "        pbar.set_description(\"Epoch {:03} Train {:.4f} Val {:.4f}\"\\\n",
        "                             .format(epoch, train_losses[-1], val_losses[-1]))\n",
        "        pbar.update(1)\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return torch.tensor(train_losses), torch.tensor(val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdRq6bR4h5Sz"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "Load the data from the HDF5 file.\n",
        "- Each file contains a `train` and `test` group.\n",
        "- Each group contains:\n",
        "    - `time`: length `frames` array of timestamps\n",
        "    - `stimulus`: a `frames x 50 x 50` video taken at ~100Hz\n",
        "    - `response`: a group with\n",
        "        - `binned`: `cells x frames` array of spike counts (for the training data) or rates (for the test data) in each bin\n",
        "        - `firing_rate_xms` where `x` is 5, 10, or 20 milliseconds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vn8xa-Md9oG",
        "outputId": "43f59fd7-80a2-4825-dfa9-68eca9540633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-28 13:17:08--  https://github.com/slinderman/ml4nd/raw/refs/heads/main/data/04_glms/lab4_data.h5\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/slinderman/ml4nd/refs/heads/main/data/04_glms/lab4_data.h5 [following]\n",
            "--2025-04-28 13:17:08--  https://media.githubusercontent.com/media/slinderman/ml4nd/refs/heads/main/data/04_glms/lab4_data.h5\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1026877736 (979M) [application/octet-stream]\n",
            "Saving to: ‘lab4_data.h5’\n",
            "\n",
            "lab4_data.h5        100%[===================>] 979.31M   155MB/s    in 7.6s    \n",
            "\n",
            "2025-04-28 13:17:32 (129 MB/s) - ‘lab4_data.h5’ saved [1026877736/1026877736]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://github.com/slinderman/ml4nd/raw/refs/heads/main/data/04_glms/lab4_data.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BK5GG1_2eLsZ"
      },
      "outputs": [],
      "source": [
        "# Load the white noise data\n",
        "f = h5py.File(\"lab4_data.h5\", mode='r')\n",
        "times = torch.tensor(f['train']['time'][:], dtype=dtype)\n",
        "stimulus = torch.tensor(f['train']['stimulus'][:], dtype=torch.uint8)\n",
        "spikes = torch.tensor(f['train']['response']['binned'][:].T, dtype=dtype)\n",
        "test_times = torch.tensor(f['test']['time'][:], dtype=dtype)\n",
        "test_stimulus = torch.tensor(f['test']['stimulus'][:], dtype=torch.uint8)\n",
        "test_rates = torch.tensor(f['test']['response']['binned'][:, :-1].T, dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AntU2cN4gQql"
      },
      "outputs": [],
      "source": [
        "# Extract/set some constants.\n",
        "NUM_FRAMES, HEIGHT, WIDTH = stimulus.shape\n",
        "_, NUM_NEURONS = spikes.shape\n",
        "FRAME_RATE = 100    # Hz\n",
        "MAX_DELAY = 40      # frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h83mfn1O8dOy"
      },
      "source": [
        "## Part 1: Plot the data\n",
        "\n",
        "Always visualize your data first!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcszNvfvQw3m"
      },
      "source": [
        "### Problem 1a: Plot a slice of the spike train\n",
        "\n",
        "Write a function to `imshow` a slice of the data.\n",
        "Add a colorbar and label your axes!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ij7EKtI5jn_Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "y7KHHJrysLwI",
        "outputId": "d3a78155-143e-41b8-e82e-8e5d09e7f1f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot a few seconds of the spike train\n",
        "def plot_spike_train(spikes: Float[Tensor, \"num_frames num_neurons\"],\n",
        "                     t_start: float,\n",
        "                     t_stop: float,\n",
        "                     figsize: Tuple[int, int]=(12, 6)\n",
        "                     ) -> None:\n",
        "    \"\"\"\n",
        "    Visualize a window of the spike count matrix.\n",
        "\n",
        "    spikes:  time x neuron spike count matrix\n",
        "    t_start: time (in seconds) of the start of the window\n",
        "    t_stop:  time (in seconds) of the end of the window\n",
        "    figsize: width and height of the figure in inches\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "    x_axis = np.linspace(t_start, t_stop, (t_stop - t_start) * FRAME_RATE)\n",
        "    plt.imshow(spikes[int(t_start * FRAME_RATE) : int(t_stop * FRAME_RATE), :].T,\n",
        "               aspect = 'auto', extent=[t_start, t_stop, 0, spikes.shape[1]],\n",
        "               interpolation = 'nearest', origin = 'lower')\n",
        "\n",
        "    plt.colorbar(label = \"N spikes\")\n",
        "\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Neurons\")\n",
        "\n",
        "    ###\n",
        "\n",
        "plot_spike_train(spikes, 0, 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spikes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABvPwSO7jCf5",
        "outputId": "e7907751-a548-4d49-bdc4-9f9f09b8b85c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([359802, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgEAmHRB0JfA"
      },
      "source": [
        "### Problem 1b: Compute the baseline firing rate for each neuron\n",
        "\n",
        "Print the mean firing rate for each neuron (on the training data) in spikes per second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_N1KmLQ1Eaj",
        "outputId": "ff55118f-0208-4b5b-d218-99dd8095c337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean firing rate: 286233.31 per sec\n",
            "Mean firing rate: 76833.33 per sec\n",
            "Mean firing rate: 79244.44 per sec\n",
            "Mean firing rate: 251699.98 per sec\n",
            "Mean firing rate: 56122.22 per sec\n",
            "Mean firing rate: 53211.11 per sec\n",
            "Mean firing rate: 73944.45 per sec\n",
            "Mean firing rate: 18677.78 per sec\n",
            "Mean firing rate: 151866.66 per sec\n"
          ]
        }
      ],
      "source": [
        "###\n",
        "# Compute the firing rates\n",
        "# YOUR CODE BELOW\n",
        "\n",
        "total_time = spikes.shape[1] / FRAME_RATE\n",
        "\n",
        "for ineuron, neuron in enumerate(spikes.T):\n",
        "\n",
        "  print(f\"Mean firing rate: {round(float(neuron.sum() / total_time), 2)} per sec\")\n",
        "\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFdCd4yoQt1Y"
      },
      "source": [
        "### Plot a few frames of the stimulus\n",
        "\n",
        "Plot the 0th, 10th, 20th, and 30th frames of stimulus in grayscale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "LGYNySW4sJPl",
        "outputId": "ee31a5f6-5a9a-4257-fe58-b1331c3030c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot a few frames of stimulus\n",
        "def plot_stimulus(stimulus, frame_inds, n_cols=4, panel_size=4):\n",
        "    num_frames = len(frame_inds)\n",
        "    n_rows = int(torch.ceil(torch.tensor(num_frames / n_cols)))\n",
        "    fig, axs = plt.subplots(\n",
        "        n_rows, n_cols, figsize=(n_cols * panel_size, n_rows * panel_size))\n",
        "    for ax, ind in zip(axs.ravel(), frame_inds):\n",
        "        ax.imshow(stimulus[ind], cmap=\"Greys\")\n",
        "        ax.set_title(\"Stimulus Frame {}\".format(ind))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    for ax in axs.ravel()[len(frame_inds):]:\n",
        "        ax.set_visible(False)\n",
        "\n",
        "plot_stimulus(stimulus, [0, 10, 20, 30])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjRhwb8Q83L"
      },
      "source": [
        "### Problem 1c: Compute and plot the spike triggered average\n",
        "\n",
        "The spike triggered average for neuron $n$ is the average stimulus in the lead-up to a spike by that neuron.\n",
        "\n",
        "Formally, let $A_n \\in \\mathbb{R}^{D \\times P_H \\times P_W}$ denote the STA for neuron $n$. It's defined as,\n",
        "\\begin{align}\n",
        "A_{n,d,i,j} = \\frac{1}{S_{n,d}} \\sum_{t=d+1}^T x_{t-d,i,j} \\mathbb{I}[y_{t,n} >0]\n",
        "\\end{align}\n",
        "where $S_{n,d} = \\sum_{t=d+1}^T \\mathbb{I}[y_{t,n} >0]$ is the number of spikes on neuron $n$, accounting for edge effects."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spikes.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwT5ZzIYoKXs",
        "outputId": "166aec4d-3e8b-4988-f6cb-19f1f7930862"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6fdmKcetfh8D",
        "outputId": "d4f661ab-4517-4730-c912-f7b741461fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "0\n",
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-83b6d8e60aa2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0msta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstimulus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspikes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mplot_sta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-83b6d8e60aa2>\u001b[0m in \u001b[0;36mcompute_sta\u001b[0;34m(neuron, stimulus, spikes, max_delay)\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0msta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimulus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mival\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mival\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_spikes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def compute_sta(neuron: int,\n",
        "                stimulus: Float[Tensor, \"num_frames height width\"],\n",
        "                spikes: Float[Tensor, \"num_frames num_neurons\"],\n",
        "                max_delay: int=25\n",
        "                ) -> Float[Tensor, \"max_delay height width\"]:\n",
        "    \"\"\"\n",
        "    Compute the spike triggered average (STA) for a specified neuron.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neuron : int\n",
        "        index of the neuron\n",
        "    stimulus : Float[Tensor, \"num_frames height width\"]\n",
        "        stimulus array\n",
        "    spikes : Float[Tensor, \"num_frames num_neurons\"]\n",
        "        spike count array\n",
        "    max_delay : int\n",
        "        number of preceding frames (D) in the STA\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Float[Tensor, \"max_delay height width\"]\n",
        "        spike triggered average (STA) for the specified neuron\n",
        "    \"\"\"\n",
        "    stim_shape = stimulus.shape[1:]\n",
        "    sta = torch.zeros((max_delay,) + stim_shape)\n",
        "\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "\n",
        "\n",
        "    print(sta.shape)\n",
        "    print(stimulus.shape)\n",
        "    print(spikes.shape)\n",
        "\n",
        "    neuron_ts = spikes.T[neuron, :]\n",
        "    print(neuron_ts.shape)\n",
        "\n",
        "    n_spikes = 0\n",
        "    for ival, val in enumerate(neuron_ts):\n",
        "\n",
        "      if val > 0:\n",
        "\n",
        "        n_spikes = n_spikes + 1\n",
        "\n",
        "        for i in range(max_delay):\n",
        "          sta[i, :, :] = sta[i, :, :] + torch.mean(stimulus[ival - 2 - i : ival - 1, :, :].float(), axis = 0)\n",
        "\n",
        "          print(i)\n",
        "\n",
        "    sta = sta / n_spikes\n",
        "\n",
        "    ###\n",
        "    return sta\n",
        "\n",
        "def plot_sta(neuron: int,\n",
        "             sta: Float[Tensor, \"max_delay height width\"],\n",
        "             n_cols: int=5\n",
        "             ) -> None:\n",
        "    \"\"\"\n",
        "    Plot the spike triggered average (STA) for a specified neuron.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    neuron : int\n",
        "        index of the neuron\n",
        "    sta : Float[Tensor, \"max_delay height width\"]\n",
        "        spike triggered average (STA) for the specified neuron\n",
        "    n_cols : int\n",
        "        number of columns in the plot\n",
        "\n",
        "    \"\"\"\n",
        "    max_delay = sta.shape[0]\n",
        "    n_rows = int(torch.ceil(torch.tensor(max_delay / n_cols)))\n",
        "\n",
        "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_rows, 4 * n_cols))\n",
        "    vmin = sta.min()\n",
        "    vmax = sta.max()\n",
        "    for d, ax in enumerate(axs.ravel()):\n",
        "        ax.imshow(sta[d], vmin=vmin, vmax=vmax, cmap=\"Greys\")\n",
        "        ax.set_axis_off()\n",
        "        ax.set_title(\"neuron {}, {}ms pre\".format(neuron + 1, d*10))\n",
        "    for ax in axs.ravel()[max_delay:]:\n",
        "        ax.set_visible(False)\n",
        "\n",
        "n = 0\n",
        "sta = compute_sta(n, stimulus, spikes)\n",
        "plot_sta(n, sta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZVrpQLkDy1c"
      },
      "source": [
        "### Finally, create PyTorch Datasets containing the stimuli and the spikes.\n",
        "Before moving onto the modeling sections, we'll split the training stimulus and spikes into batches of length 1000 frames (10 seconds of data). Then we'll randomly assign 20% of the batches to a validation dataset. We've written a simple dataset to get the training and validation batches. For stability, we normalize the stimulus to be binary rather than 0 or 128, as in the raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vwXE8V_b2mjN"
      },
      "outputs": [],
      "source": [
        "class RGCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for the RGC white noise data.\n",
        "    \"\"\"\n",
        "    stimulus: Float[Tensor, \"num_frames height width\"]\n",
        "    spikes: Float[Tensor, \"num_frames num_neurons\"]\n",
        "\n",
        "    def __init__(self,\n",
        "                 stimulus: Float[Tensor, \"num_frames height width\"],\n",
        "                 spikes: Float[Tensor, \"num_frames num_neurons\"]\n",
        "                 ) -> None:\n",
        "        self.stimulus = stimulus\n",
        "        self.spikes = spikes\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of frames in the dataset\n",
        "        return self.stimulus.shape[0]\n",
        "\n",
        "    def __getitem__(self,\n",
        "                    idx: int\n",
        "                    ) -> Dict[str, Float[Tensor, \"...\"]]:\n",
        "        # Binarize the stimulus, move it and the spikes to the GPU,\n",
        "        # and package into a dictionary\n",
        "        x = self.stimulus[idx].to(device).type(dtype) / 128.0\n",
        "        y = self.spikes[idx].to(device)\n",
        "        return dict(stimulus=x, spikes=y)\n",
        "\n",
        "def make_datasets(batch_size: int=1000\n",
        "                  ) -> Tuple[RGCDataset, RGCDataset]:\n",
        "    \"\"\"\n",
        "    Create the training and validation datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch_size : int\n",
        "        The number of frames in each batch.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[RGCDataset, RGCDataset]\n",
        "        The training and validation datasets.\n",
        "    \"\"\"\n",
        "    n_batches = NUM_FRAMES // batch_size\n",
        "    batched_stimulus = stimulus[:n_batches * batch_size]\n",
        "    batched_stimulus = batched_stimulus.reshape(n_batches, batch_size, HEIGHT, WIDTH)\n",
        "    batched_spikes = spikes[:n_batches * batch_size]\n",
        "    batched_spikes = batched_spikes.reshape(n_batches, batch_size, NUM_NEURONS)\n",
        "\n",
        "    # Split into train and validation\n",
        "    torch.manual_seed(0)\n",
        "    n_train = int(0.8 * n_batches)\n",
        "    order = torch.randperm(n_batches)\n",
        "    train_stimulus = batched_stimulus[:n_train]\n",
        "    val_stimulus = batched_stimulus[n_train:]\n",
        "    train_spikes = batched_spikes[:n_train]\n",
        "    val_spikes = batched_spikes[n_train:]\n",
        "\n",
        "    train_dataset = RGCDataset(train_stimulus, train_spikes)\n",
        "    val_dataset = RGCDataset(val_stimulus, val_spikes)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "train_dataset, val_dataset = make_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npfHppegnItd"
      },
      "source": [
        "## Part 2: Fit a linear-nonlinear Poisson (LNP) model\n",
        "\n",
        "Let's start with a simple linear-nonlinear-Poisson (LNP) model. In statistics, we would just call this a generalized linear model (GLM), but here we'll stick to the neuroscience lingo to be consistent with McIntosh et al. (2016). LNP models (and GLMs more generally) are natural models for count data, like spike counts. Whereas standard linear models could ouput negative means, these models are constrained to output non-negative expected spike counts. Moreover, since they use a Poisson noise model, the variance of the spike counts will grow with the mean, unlike in typical linear regression models.\n",
        "\n",
        "The basic LNP model is,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbb{E}[y_{t,n} \\mid \\mathbf{X}, \\mathbf{W}_n]\n",
        "&= f \\left(\\sum_{d=1}^D \\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} x_{t-d,i,j} w_{n,d,i,j} \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}_n \\in \\mathbb{R}^{D \\times P_h \\times P_W}$ are the weights, and entry $w_{n,d,i,j}$ is the weight neuron $n$ gives to the simulus at pixel $i,j$ at $d$ frames preceding the current time. Assume the weights factor into a **spatial footprint** $\\mathbf{u}_n \\in \\mathbb{R}^{P_H \\times P_W}$ times a temporal profile $\\mathbf{v}_n \\in \\mathbb{R}^D$.\n",
        "\n",
        "$$\n",
        "w_{n,d,i,j} = v_{n,d} u_{n,i,j}\n",
        "$$\n",
        "\n",
        "Then the expected value can be written as,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbb{E}[y_{t,n} \\mid X]\n",
        "&= f \\left( \\sum_{d=1}^D v_{n,d} \\left(\\sum_{i=1}^{P_H} \\sum_{j=1}^{P_W} x_{t-d,i,j} u_{n,i,j} \\right) \\right) \\\\\n",
        "&= f \\left( \\sum_{d=1}^D v_{n,d} \\tilde{x}_{n,t-d} \\right) \\\\\n",
        "&= f \\left( a_{t,n} \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "a_{t,n} = [\\tilde{\\mathbf{x}}_n \\star \\mathbf{v}_n]_t\n",
        "$$\n",
        "\n",
        "is the **activation** of neuron $n$ at time $t$. The activation is a cross-correlation (convolution in PyTorch) between $\\tilde{\\mathbf{x}}_{n} \\in \\mathbb{R}^T$, the stimulus projected onto the spatial filter for neuron $n$, and $\\mathbf{v}_n$, the temporal profile for neuron $n$. The mean function $f: \\mathbb{R} \\to \\mathbb{R}_+$ maps activation to a non-negative expected spike count.\n",
        "\n",
        "Once we compute $\\mathbb{E}[y_{t,n} \\mid \\mathbf{X}, \\mathbf{W}_n]$ we compute the likelihood function based on a Poisson regression model:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log p(y_{t,n}; \\mathbf{X} \\mathbf{W}_n) &= \\mathrm{Po}(y_{t,n}; f ( a_{t,n} )) \\\\\n",
        "&= \\log f( a_{t,n})- f ( a_{t,n} ) - \\log(y_{t,n}!)\n",
        "\\\\ & = y_{t,n} a_{t,n}- \\exp ( a_{t,n} ) - \\log(y_{t,n}!)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $f(a) = e^a$.\n",
        "\n",
        "Summing across samples in $t$ leads to the full likelihood for estimating the parameters for a given neuron. We can do this simultaneously across all neurons by summing over $n$ too, as gradient descent will independently update each neuron's parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "galqOTzc2D72"
      },
      "source": [
        "### Problem 2a: Implement the model\n",
        "\n",
        "Let's start by implementing the GLM model as a class that inherits from `nn.Module`. The  `forward` method returns the mean spike count for each time bin\n",
        "given the stimulus. In the loss function below (Problem 2b), we'll pass this output to the mean of a Poisson distribution.\n",
        "\n",
        "**Notes:**\n",
        "- As in Lab 2, you should first project the stimulus onto the spatial filters with a linear layer, then you can convolve with the temporal filters.\n",
        "- Even though the spatial projection is a linear layer, we'll call it `spatial_conv` since its a factor of a spatiotemporal convolution. This naming will also be consistent with our models below.\n",
        "- Both `spatial_conv` and `temporal_conv` include a learnable bias, by default. We only need one, so turn off the bias in the spatial layer.\n",
        "- `mean_function` specifies the mapping from the linear predictor to the expected spike count. We'll use an exponential function to be consistent with the lecture, but `F.softplus` is more common in practice. (It tends to be a little more stable during training.\n",
        "- We set the initial bias to a value that is roughly the log of the average spike count so that our initial means are in the right ballpark.\n",
        "- We'll add a small positive constant to the firing rate in the `forward` function to ensure that we don't get `log(0)` errors during training.\n",
        "- `forward` takes a keyword argument `spikes`. We won't use it in this model, but we need it here so that our training algorithm will work for this model as well as the later ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6jpvF4v_0Iam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "23bbfd18-ec13-4c48-e41b-cd22013fe106"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Linear.__init__() missing 1 required positional argument: 'out_features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-82b0e9d77f8e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# Fix the seed so that the tests below will work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mlnp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLNP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mcheck_model_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlnp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-82b0e9d77f8e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_neurons, height, width, max_delay, mean_function, initial_bias)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# YOUR CODE BELOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Linear.__init__() missing 1 required positional argument: 'out_features'"
          ]
        }
      ],
      "source": [
        "class LNP(nn.Module):\n",
        "    \"\"\"\n",
        "    A linear-nonlinear-Poisson (LNP) model for RGC data, as described above.\n",
        "\n",
        "    The model consists of a linear filter (spatial and temporay convolution) followed by\n",
        "    a nonlinear function to produce the mean of a Poisson spike count distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_neurons : int\n",
        "        The number of neurons in the model.\n",
        "    height : int\n",
        "        The height of the stimulus in pixels.\n",
        "    width : int\n",
        "        The width of the stimulus in pixels.\n",
        "    max_delay : int\n",
        "        The maximum delay in frames for the temporal convolution.\n",
        "    mean_function : callable\n",
        "        The function to compute the mean of the Poisson distribution.\n",
        "    initial_bias : float\n",
        "        The initial bias for the temporal convolution.\n",
        "    spatial_conv : nn.Linear\n",
        "        The linear layer for the spatial convolution.\n",
        "    temporal_conv : nn.Conv1d\n",
        "        The convolutional layer for the temporal convolution.\n",
        "    \"\"\"\n",
        "    num_neurons: int\n",
        "    height: int\n",
        "    width: int\n",
        "    max_delay: int\n",
        "    mean_function: callable\n",
        "    initial_bias: float\n",
        "    spatial_conv: nn.Linear\n",
        "    temporal_conv: nn.Conv1d\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_neurons=NUM_NEURONS,\n",
        "                 height=HEIGHT,\n",
        "                 width=WIDTH,\n",
        "                 max_delay=MAX_DELAY,\n",
        "                 mean_function=torch.exp,\n",
        "                 initial_bias=0.05):\n",
        "        super(LNP, self).__init__()\n",
        "        self.num_neurons = num_neurons\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.max_delay = max_delay\n",
        "        self.mean_function = mean_function\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE BELOW\n",
        "        #\n",
        "        self.spatial_conv = nn.Linear(...)\n",
        "        self.temporal_conv = nn.Conv1d(...)\n",
        "        #\n",
        "        ###\n",
        "\n",
        "        # Initialize the bias\n",
        "        torch.nn.init.constant_(self.temporal_conv.bias,\n",
        "                                torch.log(torch.tensor(initial_bias)))\n",
        "\n",
        "    def forward(self,\n",
        "                stimulus: Float[Tensor, \"num_frames height width\"],\n",
        "                spikes: Optional[Float[Tensor, \"num_frames num_neurons\"]]=None\n",
        "                ) -> Float[Tensor, \"num_frames num_neurons\"]:\n",
        "        \"\"\"\n",
        "        Compute the expected spike counts for a given stimulus.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stimulus : Float[Tensor, \"num_frames height width\"]\n",
        "            The stimulus to be processed.\n",
        "        spikes : Float[Tensor, \"num_frames num_neurons\"], optional\n",
        "            The spike counts for the neurons. This is NOT USED in the model,\n",
        "            but is included for compatibility with the training loop.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"num_frames num_neurons\"]\n",
        "            The expected spike counts for the neurons.\n",
        "\n",
        "        \"\"\"\n",
        "        ###\n",
        "        # YOUR CODE BELOW\n",
        "        #\n",
        "        x = stimulus\n",
        "        ...\n",
        "        ###\n",
        "\n",
        "        return 1e-4 + x\n",
        "\n",
        "\n",
        "def check_model_outputs(model):\n",
        "    \"\"\"\n",
        "    Check that the model outputs are the right shape and non-negative.\n",
        "    \"\"\"\n",
        "    out = model(train_dataset[0]['stimulus'],\n",
        "                train_dataset[0]['spikes'])\n",
        "    assert out.shape == train_dataset[0]['spikes'].shape\n",
        "    assert torch.all(out > 0)\n",
        "\n",
        "# Construct an LNP model with random initial weights.\n",
        "# Fix the seed so that the tests below will work\n",
        "torch.manual_seed(0)\n",
        "lnp = LNP().to(device)\n",
        "check_model_outputs(lnp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8kdQJmogGI_"
      },
      "source": [
        "### Problem 2b: Implement the Poisson loss\n",
        "Compute the average negative log likelihood of the spikes (taking the mean over neurons and frames) given the expected spike counts (`rates`) ouput by the model.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathcal{L}(\\mathbf{W}) = -\\frac{1}{NT} \\sum_{n=1}^N \\sum_{t=1}^T \\log \\mathrm{Po}(y_{t,n} \\mid f(a_{t,n})\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwzhpHNQI73j"
      },
      "outputs": [],
      "source": [
        "def poisson_loss(rate: Float[Tensor, \"num_frames num_neurons\"],\n",
        "                 spikes: Float[Tensor, \"num_frames num_neurons\"]):\n",
        "    \"\"\"Compute the log-likelihood under a Poisson spiking model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rate: Float[Tensor, \"num_frames num_neurons\"]\n",
        "        The expected spike counts for the neurons.\n",
        "    spikes: Float[Tensor, \"num_frames num_neurons\"]\n",
        "        The actual spike counts for the neurons.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Float[Tensor, \"num_frames num_neurons\"]\n",
        "        The negative log likelihood of the Poisson distribution.\n",
        "        This is the loss function for the LNP and subsequent GLM models.\n",
        "\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    avg_nll = ...\n",
        "    ###\n",
        "\n",
        "    return avg_nll\n",
        "\n",
        "assert torch.isclose(\n",
        "    poisson_loss(lnp(train_dataset[0]['stimulus']),\n",
        "                 train_dataset[0]['spikes']),\n",
        "    torch.tensor(0.2675), atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFip8dPMQVXk"
      },
      "source": [
        "### Problem 2c: Add $\\ell_2$ weight regularization\n",
        "\n",
        "To the Poisson loss above, we'll add a regularization penalty on the squared $\\ell_2$ norm of the weights,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathcal{R}(\\mathbf{W}) &= \\frac{\\alpha}{2} \\sum_{n=1}^N (\\|\\mathbf{u}_n\\|_F^2 + \\|\\mathbf{v}_n\\|_F^2)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{u}_n$ and $\\mathbf{v}_n$ are the spatial and temporal weights for neuron $n$, respectively, and $\\alpha$ is a scaling factor.\n",
        "\n",
        "Do not regularize the biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdP8kBeXBcY3"
      },
      "outputs": [],
      "source": [
        "def lnp_regularizer(model: LNP,\n",
        "                    alpha: float=1e-3\n",
        "                    ) -> float:\n",
        "    \"\"\"\n",
        "    Compute the log prior probability under a mean-zero Gaussian model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: LNP\n",
        "        The LNP model to be regularized.\n",
        "    alpha: float\n",
        "        The regularization strength.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The regularization term for the model. This is the sum of the\n",
        "        squared weights of the spatial and temporal convolution filters.\n",
        "\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    reg = ...\n",
        "    ###\n",
        "\n",
        "    return reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZvRQVcQhoIY"
      },
      "source": [
        "### Fit the LNP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb0YF6BnhrWy"
      },
      "outputs": [],
      "source": [
        "# Construct an LNP model with random initial weights.\n",
        "torch.manual_seed(0)\n",
        "lnp = LNP().to(device)\n",
        "\n",
        "# Fit the LNP model\n",
        "print(\"Training LNP model. This should take about 2 minutes...\")\n",
        "train_losses, val_losses = \\\n",
        "    train_model(lnp,\n",
        "                train_dataset,\n",
        "                val_dataset,\n",
        "                poisson_loss,\n",
        "                lnp_regularizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjPchkKjEAXk"
      },
      "source": [
        "### Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OvCUpMQ9jQC"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
        "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
        "axs[0].set_xlabel(\"epoch\")\n",
        "axs[0].set_ylabel(\"poisson loss\")\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(loc=\"upper right\")\n",
        "\n",
        "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
        "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
        "axs[1].set_xlabel(\"epoch\")\n",
        "axs[1].set_ylabel(\"poisson loss\")\n",
        "axs[1].set_ylim(top=val_losses[20])\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03g-_aoE06tj"
      },
      "outputs": [],
      "source": [
        "plot_stimulus_weights(lnp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xABSxqSLWNI8"
      },
      "source": [
        "### Problem 2d: [Short Answer] Interpret the results\n",
        "\n",
        "How do the spatiotemporal filters relate to the STA from Problem 1c? Are they mathematically related?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P2uD55Iizjk"
      },
      "source": [
        "\n",
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2uyT_J3up4u"
      },
      "source": [
        "## Part 3: Fit a GLM with inter-neuron couplings\n",
        "\n",
        "Now add inter-neuron couplings to the basic model above. For historical reasons, the LNP with inter-neuron couplings is what some neuroscientists call a GLM, even though they're both instances of generalized linear models! Again, we're just going to stick to the notation of McIntosh et al (2016) for this lab anyway.  \n",
        "\n",
        "The new model has activation,\n",
        "\n",
        "$$\n",
        "a_{t,n}\n",
        "= [\\tilde{\\mathbf{x}}_n \\star \\mathbf{v}_n]_t +  \\sum_{m=1}^N \\sum_{d=1}^D y_{t-d,m} g_{m,n,d}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{G} \\in \\mathbb{R}^{N \\times N \\times D}$ is a tensor of **coupling** weights.\n",
        "\n",
        "You can implement the activation using a convolution of $\\mathbf{Y}$ and $\\mathbf{G}$.\n",
        "\n",
        "**Note:** as above, the `coupling_conv` will have a bias by default. Get rid of it. You don't need it since there's already a bias in the `temporal_conv`.\n",
        "\n",
        "**IMPORTANT:** Make sure your output only depends on spike counts up to but _not including_ time $t$!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL9nCdv3yuTx"
      },
      "source": [
        "### Problem 3a: Implement the coupled model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McoqFbRkp4s2"
      },
      "outputs": [],
      "source": [
        "class GLM(nn.Module):\n",
        "    \"\"\"\n",
        "    A generalized linear model (GLM) with a spatiotemporal filter\n",
        "    and coupling weights. The model consists of a linear filter (spatial and\n",
        "    temporal convolution) followed by a nonlinear function to produce the mean\n",
        "    of a Poisson spike count distribution. The model also includes coupling\n",
        "    weights to account for the influence of other neurons on the firing rate\n",
        "    of a neuron.\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_neurons : int\n",
        "        The number of neurons in the model.\n",
        "    height : int\n",
        "        The height of the stimulus in pixels.\n",
        "    width : int\n",
        "        The width of the stimulus in pixels.\n",
        "    max_delay : int\n",
        "        The maximum delay in frames for the temporal convolution.\n",
        "    mean_function : callable\n",
        "        The function to compute the mean of the Poisson distribution.\n",
        "    initial_bias : float\n",
        "        The initial bias for the temporal convolution.\n",
        "    spatial_conv : nn.Linear\n",
        "        The linear layer for the spatial convolution.\n",
        "    temporal_conv : nn.Conv1d\n",
        "        The convolutional layer for the temporal convolution.\n",
        "    coupling_conv : nn.Conv1d\n",
        "        The convolutional layer for the coupling weights.\n",
        "    \"\"\"\n",
        "    num_neurons: int\n",
        "    height: int\n",
        "    width: int\n",
        "    max_delay: int\n",
        "    mean_function: callable\n",
        "    spatial_conv: nn.Linear\n",
        "    temporal_conv: nn.Conv1d\n",
        "    coupling_conv: nn.Conv1d\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_neurons: int=NUM_NEURONS,\n",
        "                 height: int=HEIGHT,\n",
        "                 width: int=WIDTH,\n",
        "                 max_delay: int=MAX_DELAY,\n",
        "                 initial_bias: float=0.05,\n",
        "                 mean_function: callable=torch.exp\n",
        "                 ) -> None:\n",
        "        super(GLM, self).__init__()\n",
        "        self.num_neurons = num_neurons\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.max_delay = max_delay\n",
        "        self.mean_function = mean_function\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE BELOW\n",
        "        #\n",
        "        self.spatial_conv = nn.Linear(...)\n",
        "        self.temporal_conv = nn.Conv1d(...)\n",
        "        self.coupling_conv = nn.Conv1d(...)\n",
        "        ###\n",
        "\n",
        "        # Initialize the bias\n",
        "        torch.nn.init.constant_(self.temporal_conv.bias,\n",
        "                                torch.log(torch.tensor(initial_bias)))\n",
        "\n",
        "    def forward(self,\n",
        "                stimulus: Float[Tensor, \"num_frames height width\"],\n",
        "                spikes: Float[Tensor, \"num_frames num_neurons\"]\n",
        "                ) -> Float[Tensor, \"num_frames num_neurons\"]:\n",
        "        \"\"\"\n",
        "        Compute the expected spike counts for a given stimulus and spikes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stimulus: Float[Tensor, \"num_frames height width\"]\n",
        "            The stimulus to be processed.\n",
        "        spikes: Float[Tensor, \"num_frames num_neurons\"]\n",
        "            The spike counts for the neurons. This is used to compute the\n",
        "            coupling weights.\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"num_frames num_neurons\"]\n",
        "            The expected spike counts for the neurons.\n",
        "\n",
        "        \"\"\"\n",
        "        x, y = stimulus, spikes\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE BELOW\n",
        "        rate = ...\n",
        "        ###\n",
        "\n",
        "        # Apply the nonlinearity and add small positive bias\n",
        "        return 1e-4 + rate\n",
        "\n",
        "# Construct a coupled GLM model with random initial weights.\n",
        "torch.manual_seed(0)\n",
        "glm = GLM().to(device)\n",
        "check_model_outputs(glm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLjycnQuy8Nj"
      },
      "source": [
        "### Problem 3b: Implement a regularizer for the coupled GLM weights\n",
        "\n",
        "Put an $\\ell_2$ penalty on the weights of the `spatial_conv`, `temporal_conv`, and `coupling_conv`. No need to regularize the bias. Scale the regularization by $\\alpha$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_bmk5eqsvCF"
      },
      "outputs": [],
      "source": [
        "def glm_regularizer(model: GLM,\n",
        "                    alpha: float=1e-3\n",
        "                    ) -> float:\n",
        "    \"\"\"\n",
        "    Implement an \\ell_2 penalty on the norm of the model weights,\n",
        "    as described above.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: GLM\n",
        "        The GLM model to be regularized.\n",
        "    alpha: float\n",
        "        The regularization strength.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The regularization term for the model. This is the sum of the\n",
        "        squared weights of the spatial, temporal, and coupling convolution\n",
        "        filters.\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    reg = ...\n",
        "    ###\n",
        "\n",
        "    return reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYywYbtitPQZ"
      },
      "source": [
        "### Fit the GLM model with couplings between neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbC-Y1BhtOkb"
      },
      "outputs": [],
      "source": [
        "# Construct a coupled GLM model with random initial weights.\n",
        "torch.manual_seed(0)\n",
        "glm = GLM().to(device)\n",
        "\n",
        "# Fit the model\n",
        "print(\"Training coupled GLM. This should take about 2-4 minutes...\")\n",
        "train_losses, val_losses = \\\n",
        "    train_model(glm,\n",
        "                train_dataset,\n",
        "                val_dataset,\n",
        "                poisson_loss,\n",
        "                glm_regularizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOV3xBHAgkHU"
      },
      "source": [
        "### Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7RtWo53CMi9"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
        "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
        "axs[0].set_xlabel(\"epoch\")\n",
        "axs[0].set_ylabel(\"poisson loss\")\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(loc=\"upper right\")\n",
        "\n",
        "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
        "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
        "axs[1].set_xlabel(\"epoch\")\n",
        "axs[1].set_ylabel(\"poisson loss\")\n",
        "axs[1].set_ylim(top=val_losses[20])\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEviXKwBep5m"
      },
      "outputs": [],
      "source": [
        "plot_stimulus_weights(glm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG-oC8mmevo_"
      },
      "outputs": [],
      "source": [
        "plot_coupling_weights(glm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjk8WqT5W5gB"
      },
      "source": [
        "### Problem 3c: [Short Answer] Interpret the results\n",
        "\n",
        "Did adding the coupling weights change the spatiotemporal stimulus filters in any perceptible way? Do you see any interesting structure in the coupling weights? What other regularization strategies could you have applied to the coupling weights?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKh7IrfLq4Kd"
      },
      "source": [
        "\n",
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSpKiGqFv8U6"
      },
      "source": [
        "## Part 4: Convolutional neural network model\n",
        "\n",
        "Finally, we'll implement a convolutional neural network like the one proposed in McIntosh et al (2016). (See above.) We'll make some slight modifications though, so that the model doesn't take so long to fit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faWvElPLYqgD"
      },
      "source": [
        "### Problem 4a: Implement the convolutional model\n",
        "\n",
        "Implement the following model:\n",
        "\n",
        "1. **Apply a rank-1 spatiotemporal filter to the video:**\n",
        "\n",
        "    a.  First convolve with 2D receptive fields of size `rf_size_1` and `num_subunits_1` output channels. You do not need to pad the edges since the neurons respond primarily to the center of the video. Your output should be `T x N1 x H1 x W1` where `T` is the number of frames, `N1` is the number of subunits, and `H1,W1` are the height and width after 2D convolution without padding.\n",
        "\n",
        "    b.  Then convolve each subunit and pixel with a temporal filter, to get another `T x N1 x H1 x W1` output.\n",
        "\n",
        "    c.  Apply a rectifying nonlinearity (`F.relu`).\n",
        "\n",
        "2. **Spatial convolution and mixing**\n",
        "\n",
        "    a.  Apply a spatial convolution of size `rf_size_2` with `num_subunits_2` output channels. This layer mixes the subunits from the first layer to obtain a representation that is, hopefully, somewhat similar to that of intermediate cells in the retina.\n",
        "    \n",
        "    b.  Apply another rectifying nonlinearity (`F.relu`). The output should be `T x N2 x H2 x W2` where `N2` is the number of subunits in the second layer and `H2,W2` are the size of the image after convolution without padding.\n",
        "\n",
        "3. **Predict expected spike counts**\n",
        "\n",
        "    a. Apply a linear read-out to the `N2 x H2 x W2` representation and pass through the mean function to obtain a `T x N` tensor of expected spike counts, where `N` is the number of neurons.\n",
        "\n",
        "\n",
        "**Notes:** The modifications we made are\n",
        "\n",
        "- We used slightly larger receptive field sizes. This actually speeds things up since, with valid padding, we end up with fewer \"pixels\" in subsequent layers.\n",
        "\n",
        "- We used a smaller number of subunits (4/4 as opposed to 8/16). This is a smaller dataset (only 9 neurons) and we seemed to overfit with more layers.\n",
        "\n",
        "- We use an exponential mean function to be consistent with the models above. Again, a softplus is more common in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-DUYEogxe-8"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional neural network (CNN) model for RGC data, as described above.\n",
        "    The model consists of a series of convolutional layers followed by a\n",
        "    fully connected layer to produce the mean of a Poisson spike count\n",
        "    distribution. The model also includes a nonlinear activation function\n",
        "    between layers to capture nonlinear features of the stimulus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_neurons : int\n",
        "        The number of neurons in the model.\n",
        "    height : int\n",
        "        The height of the stimulus in pixels.\n",
        "    width : int\n",
        "        The width of the stimulus in pixels.\n",
        "    max_delay : int\n",
        "        The maximum delay in frames for the temporal convolution.\n",
        "    rf_size_1 : int\n",
        "        The size of the receptive field for the first layer.\n",
        "    rf_size_2 : int\n",
        "        The size of the receptive field for the second layer.\n",
        "    num_subunits_1 : int\n",
        "        The number of subunits in the first layer.\n",
        "    num_subunits_2 : int\n",
        "        The number of subunits in the second layer.\n",
        "    mean_function : callable\n",
        "        The function to compute the mean of the Poisson distribution.\n",
        "    initial_bias : float\n",
        "        The initial bias for the temporal convolution.\n",
        "    spatial_conv : nn.Conv2d\n",
        "        The convolutional layer for the spatial convolution.\n",
        "    temporal_conv : nn.Conv1d\n",
        "        The convolutional layer for the temporal convolution.\n",
        "    layer2 : nn.Conv2d\n",
        "        The convolutional layer for the second layer.\n",
        "    layer3 : nn.Linear\n",
        "        The fully connected layer for the output.\n",
        "    \"\"\"\n",
        "    num_neurons: int\n",
        "    height: int\n",
        "    width: int\n",
        "    max_delay: int\n",
        "    rf_size_1: int\n",
        "    rf_size_2: int\n",
        "    num_subunits_1: int\n",
        "    num_subunits_2: int\n",
        "    mean_function: callable\n",
        "    spatial_conv: nn.Conv2d\n",
        "    temporal_conv: nn.Conv1d\n",
        "    layer2: nn.Conv2d\n",
        "    layer3: nn.Linear\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_neurons: int=NUM_NEURONS,\n",
        "                 height: int=HEIGHT,\n",
        "                 width: int=WIDTH,\n",
        "                 rf_size_1: int=21,\n",
        "                 rf_size_2: int=15,\n",
        "                 max_delay: int=MAX_DELAY,\n",
        "                 num_subunits_1: int=4,\n",
        "                 num_subunits_2: int=4,\n",
        "                 initial_bias: float=0.05,\n",
        "                 mean_function: callable=torch.exp):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        self.num_neurons = num_neurons\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.max_delay = max_delay\n",
        "        self.rf_size_1 = rf_size_1\n",
        "        self.rf_size_2 = rf_size_2\n",
        "        self.num_subunits_1 = num_subunits_1\n",
        "        self.num_subunits_2 = num_subunits_2\n",
        "        self.mean_function = mean_function\n",
        "\n",
        "        ###\n",
        "        # YOUR CODE BELOW\n",
        "        #\n",
        "        self.spatial_conv = nn.Conv2d(...)\n",
        "        self.temporal_conv = nn.Conv1d(...)\n",
        "        self.layer2 = nn.Conv2d(...)\n",
        "        self.layer3 = nn.Linear(...)\n",
        "        #\n",
        "        ###\n",
        "\n",
        "        # Initialize the bias\n",
        "        torch.nn.init.constant_(self.layer3.bias,\n",
        "                                torch.log(torch.tensor(initial_bias)))\n",
        "\n",
        "    def forward(self,\n",
        "                stimulus: Float[Tensor, \"num_frames height width\"],\n",
        "                spikes: Optional[Float[Tensor, \"num_frames num_neurons\"]]=None\n",
        "                ) -> Float[Tensor, \"num_frames num_neurons\"]:\n",
        "        \"\"\"\n",
        "        Compute the expected spike counts for a given stimulus and spikes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stimulus: Float[Tensor, \"num_frames height width\"]\n",
        "            The stimulus to be processed.\n",
        "        spikes: Float[Tensor, \"num_frames num_neurons\"]\n",
        "            The spike counts for the neurons. This is NOT USED in this model,\n",
        "            but is included for compatibility with the training loop.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Float[Tensor, \"num_frames num_neurons\"]\n",
        "            The expected spike counts for the neurons.\n",
        "\n",
        "        \"\"\"\n",
        "        x = stimulus\n",
        "        ###\n",
        "        # YOUR CODE BELOW\n",
        "        rate = ...\n",
        "        ###\n",
        "\n",
        "        return 1e-4 + rate\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "cnn = CNN().to(device)\n",
        "check_model_outputs(cnn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKOZi-lZLREY"
      },
      "source": [
        "### Problem 4b: Regularize the weights\n",
        "\n",
        "Put an $\\ell_2$ penalty on the weights of `spatial_conv`, `temporal_conv`, `layer2`, and `layer3`. Scale the regularize by $\\alpha$, as in the preceding sections. No need to regularize the biases. We found that a smaller value of $\\alpha$ was helpful, so here we default to `1e-5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLSd4T5rHKyL"
      },
      "outputs": [],
      "source": [
        "# Regularize the weights of the CNN\n",
        "def cnn_regularizer(model: CNN,\n",
        "                    alpha: float=1e-5\n",
        "                    ) -> float:\n",
        "    \"\"\"\n",
        "    Implement an \\ell_2 penalty on the norm of the model weights,\n",
        "    as described above.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: CNN\n",
        "        The CNN model to be regularized\n",
        "    alpha: float\n",
        "        The regularization strength.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The regularization term for the objective. This is the sum of the\n",
        "        squared weights of the spatial, temporal, and coupling convolution\n",
        "        filters.\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # YOUR CODE BELOW\n",
        "    reg = ...\n",
        "    #\n",
        "    ###\n",
        "\n",
        "    return reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNJj40mULrFa"
      },
      "source": [
        "### Fit the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg9epbov_2jQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "cnn = CNN().to(device)\n",
        "\n",
        "print(\"Fitting the CNN model. This should take about 10-20 minutes.\")\n",
        "train_losses, val_losses = \\\n",
        "    train_model(cnn,\n",
        "                train_dataset,\n",
        "                val_dataset,\n",
        "                poisson_loss,\n",
        "                cnn_regularizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBBKfnSyENd8"
      },
      "source": [
        "### Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxIFl9tGSVz7"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].plot(train_losses, color=palette[0], label=\"train\")\n",
        "axs[0].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
        "axs[0].set_xlabel(\"epoch\")\n",
        "axs[0].set_ylabel(\"poisson loss\")\n",
        "axs[0].grid(True)\n",
        "axs[0].legend(loc=\"upper right\")\n",
        "\n",
        "axs[1].plot(train_losses, color=palette[0], label=\"train\")\n",
        "axs[1].plot(val_losses, color=palette[1], ls='--', label=\"validation\")\n",
        "axs[1].set_xlabel(\"epoch\")\n",
        "axs[1].set_ylabel(\"poisson loss\")\n",
        "axs[1].set_ylim(top=val_losses[10])\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euh6Qpao6qe2"
      },
      "source": [
        "### Plot the subunit weights for the CNN\n",
        "\n",
        "First we'll plot the spatiotemporal filters of the first layer of subunits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drsWoCfJ4_yV"
      },
      "outputs": [],
      "source": [
        "plot_cnn_subunits_1(cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPqMs_u5MkYM"
      },
      "source": [
        "### Plot the spatial weights for the second layer of subunits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7G8BFgI4vIE"
      },
      "outputs": [],
      "source": [
        "plot_cnn_subunits2(cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N6nOINb8E9w"
      },
      "source": [
        "### Problem 4c: Predict test firing rates\n",
        "\n",
        "Finally, take the fitted models from Parts 2-4 and evaluate them on test data.\n",
        "\n",
        "The test data consists of _expected_ spike counts rather than spike counts. That's because they showed the same visual stimulus many times and computed the average response.  If our models are working well, they should output a similar firing rate in response to that same visual stimulus.\n",
        "\n",
        "**Note:** technically the coupled GLM from Part 3 expects preceding spikes as input, but here we'll give it the rates as input instead. (We don't have test spikes to feed in.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgfQzkXs8VK3"
      },
      "outputs": [],
      "source": [
        "# Move the test stimulus and measured rates to the GPU\n",
        "test_stimulus_cuda = test_stimulus.to(device).type(dtype) / 128.0\n",
        "test_rates_cuda = test_rates.to(device)\n",
        "\n",
        "###\n",
        "# YOUR CODE BELOW\n",
        "#\n",
        "lnp_test_rates = ...\n",
        "glm_test_rates = ...\n",
        "cnn_test_rates = ...\n",
        "\n",
        "# Plot a slice of the true and predicted firing rates\n",
        "...\n",
        "#\n",
        "###\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQEng0FgNBzG"
      },
      "source": [
        "### Problem 4d: Model comparison\n",
        "\n",
        "Make a bar plot of the mean squared error between the true and predicted rates for each model. As a baseline, compute the mean squared error of a constant-rate model with rate equal to the expected spike count under the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBDTFUTm_HlN"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# YOUR CODE BELOW\n",
        "#\n",
        "mse_const = ...\n",
        "mse_lnp = ...\n",
        "mse_glm = ...\n",
        "mse_cnn = ...\n",
        "#\n",
        "###\n",
        "\n",
        "# Make a bar plot\n",
        "plt.bar(0, mse_const, color='gray', ec='k')\n",
        "plt.bar(1, mse_lnp, color=palette[0], ec='k')\n",
        "plt.bar(2, mse_glm, color=palette[1], ec='k')\n",
        "plt.bar(3, mse_cnn, color=palette[2], ec='k')\n",
        "plt.xticks([0, 1, 2, 3], [\"Const.\", \"LNP\", \"GLM\", \"CNN\"])\n",
        "plt.ylabel(\"Test MSE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsadGfnrCO5o"
      },
      "source": [
        "## Part 5: Discussion\n",
        "\n",
        "You've now developed and fit three encoding models for these retinal ganglion cell responses, and hopefully you've developed some intuition for how these models work! Let's end by discussing some of the decisions that go into building and checking these models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH60uICPobwL"
      },
      "source": [
        "### Problem 5a\n",
        "All three models were fit with a Poisson loss, which has unit dispersion. What is overdispersion of count data? Is this an issue here?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdVWp1Iaol-M"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgmXy_ZwolA8"
      },
      "source": [
        "### Problem 5b\n",
        "The CNN was loosely motivated as an approximation to the layers of photoreceptors, bipolar cells, etc. that precede retinal ganglion cells. Of course, the actual circuitry is more complicated. What could you imagine adding to this model to make it more realistic?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Q_Y1phornk"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLruEbXWorAh"
      },
      "source": [
        "### Problem 5c\n",
        "In our hands, the CNN outperformed the LNP and GLM. Though it's tempting to just say the CNN is a more flexible model, notice that the CNN does not have coupling filters and it compresses the input substantially before the final read-out layer. Given the results above, what follow-up experiments would you do to further understand the root of these performance differences?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe7F1Bgcov1e"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCL51swyoviG"
      },
      "source": [
        "### Problem 5d\n",
        "We didn't ask you to do a thorough hyperparameter search. If you were to do one, what are the key parameters you would vary to try to improve model performance?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zO70J0ozAM"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzqI_1IzoyvS"
      },
      "source": [
        "### Problem 5e\n",
        "\n",
        "We fit all of these models to RGC responses to a binary white noise stimulus. Would you expect your results to change if the cells had been shown a movie with natural scenes instead?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjus5y__o26D"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pohv6Wado5Li"
      },
      "source": [
        "## Author contributions\n",
        "\n",
        "Write a short paragraph describing how each team member contributed to this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH5e9UTBo_Rj"
      },
      "source": [
        "_Your answer here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjisiMLhQxaN"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "\n",
        "Download your notebook in .ipynb format and use the following command to convert it to PDF\n",
        "```\n",
        "jupyter nbconvert --to pdf lab4_teamname.ipynb\n",
        "```\n",
        "If you're using Anaconda for package management, you can install `nbconvert` with\n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "Upload your .pdf file to Gradescope.\n",
        "\n",
        "**Only one submission per team!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}